{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AI Innovation Square 릴레이 온라인 특강: Python] \n",
    "\n",
    "\n",
    "## 10. 함수형 패러다임(2)\n",
    "\n",
    "\n",
    "- Functional Paradigm (FP)\n",
    "- 함수를 수학함수로 생각하고 수학처럼 프로그래밍\n",
    "- 함수 :first class object \n",
    "\n",
    "\n",
    "> 책추천: Clean Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x) #function의 인스턴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a=input()\n",
    "b=int(a)\n",
    "c=[]\n",
    "for i in range(b): #for 문\n",
    "    c.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- comprehension으로 간결하게\n",
    "    - list, dict, sets comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in range(int(input()))] #list comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator expression\n",
    "- tuple comp\n",
    "- yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7ffad82a72d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x for x in range(int(input())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### recursion\n",
    "- 수식표현 가능하면 구현 가능\n",
    "- 파이썬에서 느려서 사용하지 않음\n",
    "- 메모리 소비 큼\n",
    "\n",
    "##### list processing(Lisp)\n",
    "- 동시에 여러개 값 처리\n",
    "- immutable (eg. tensorflow, pytorch)\n",
    "\n",
    "\n",
    "\n",
    "- side-effect 없음\n",
    "- 문보다 식(epxression)\n",
    "- what > how\n",
    "- 파이썬은 멀티패러다임 언어\n",
    "\n",
    "### Encapsulation 캠슐화\n",
    "- 접근 제한 \n",
    "    - python class는 외부에서 내부 접근 가능\n",
    "\n",
    "### callable\n",
    "- class, function , `__call__` 이 내부에 정의된 인스턴스\n",
    "- () 붙일 수 있어 함수처럼 사용\n",
    "> function,클래스, 클래스의 instance에 `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1) #int 인스턴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(int)#int는  class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callable(int) #int class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int() #x=0 기본값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf.keras.layers.Dense) #클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7ffad5ae2550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Dense(10,activation='softmax') #인스턴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Layer(module.Module, version_utils.LayerVersionSelector):\n",
      "  \"\"\"This is the class from which all layers inherit.\n",
      "\n",
      "  A layer is a callable object that takes as input one or more tensors and\n",
      "  that outputs one or more tensors. It involves *computation*, defined\n",
      "  in the `call()` method, and a *state* (weight variables), defined\n",
      "  either in the constructor `__init__()` or in the `build()` method.\n",
      "\n",
      "  Users will just instantiate a layer and then treat it as a callable.\n",
      "\n",
      "  Arguments:\n",
      "    trainable: Boolean, whether the layer's variables should be trainable.\n",
      "    name: String name of the layer.\n",
      "    dtype: The dtype of the layer's computations and weights (default of\n",
      "      `None` means use `tf.keras.backend.floatx` in TensorFlow 2, or the type\n",
      "      of the first input in TensorFlow 1).\n",
      "    dynamic: Set this to `True` if your layer should only be run eagerly, and\n",
      "      should not be used to generate a static computation graph.\n",
      "      This would be the case for a Tree-RNN or a recursive network,\n",
      "      for example, or generally for any layer that manipulates tensors\n",
      "      using Python control flow. If `False`, we assume that the layer can\n",
      "      safely be used to generate a static computation graph.\n",
      "\n",
      "  Attributes:\n",
      "    name: The name of the layer (string).\n",
      "    dtype: The dtype of the layer's computations and weights. If mixed\n",
      "      precision is used with a `tf.keras.mixed_precision.experimental.Policy`,\n",
      "      this is instead just the dtype of the layer's weights, as the computations\n",
      "      are done in a different dtype.\n",
      "    trainable_weights: List of variables to be included in backprop.\n",
      "    non_trainable_weights: List of variables that should not be\n",
      "      included in backprop.\n",
      "    weights: The concatenation of the lists trainable_weights and\n",
      "      non_trainable_weights (in this order).\n",
      "    trainable: Whether the layer should be trained (boolean), i.e. whether\n",
      "      its potentially-trainable weights should be returned as part of\n",
      "      `layer.trainable_weights`.\n",
      "    input_spec: Optional (list of) `InputSpec` object(s) specifying the\n",
      "      constraints on inputs that can be accepted by the layer.\n",
      "\n",
      "  We recommend that descendants of `Layer` implement the following methods:\n",
      "\n",
      "  * `__init__()`: Defines custom layer attributes, and creates layer state\n",
      "    variables that do not depend on input shapes, using `add_weight()`.\n",
      "  * `build(self, input_shape)`: This method can be used to create weights that\n",
      "    depend on the shape(s) of the input(s), using `add_weight()`. `__call__()`\n",
      "    will automatically build the layer (if it has not been built yet) by\n",
      "    calling `build()`.\n",
      "  * `call(self, *args, **kwargs)`: Called in `__call__` after making sure\n",
      "    `build()` has been called. `call()` performs the logic of applying the\n",
      "    layer to the input tensors (which should be passed in as argument).\n",
      "    Two reserved keyword arguments you can optionally use in `call()` are:\n",
      "      - `training` (boolean, whether the call is in\n",
      "        inference mode or training mode)\n",
      "      - `mask` (boolean tensor encoding masked timesteps in the input, used\n",
      "        in RNN layers)\n",
      "  * `get_config(self)`: Returns a dictionary containing the configuration used\n",
      "    to initialize this layer. If the keys differ from the arguments\n",
      "    in `__init__`, then override `from_config(self)` as well.\n",
      "    This method is used when saving\n",
      "    the layer or a model that contains this layer.\n",
      "\n",
      "  Examples:\n",
      "\n",
      "  Here's a basic example: a layer with two variables, `w` and `b`,\n",
      "  that returns `y = w . x + b`.\n",
      "  It shows how to implement `build()` and `call()`.\n",
      "  Variables set as attributes of a layer are tracked as weights\n",
      "  of the layers (in `layer.weights`).\n",
      "\n",
      "  ```python\n",
      "  class SimpleDense(Layer):\n",
      "\n",
      "    def __init__(self, units=32):\n",
      "        super(SimpleDense, self).__init__()\n",
      "        self.units = units\n",
      "\n",
      "    def build(self, input_shape):  # Create the state of the layer (weights)\n",
      "      w_init = tf.random_normal_initializer()\n",
      "      self.w = tf.Variable(\n",
      "          initial_value=w_init(shape=(input_shape[-1], self.units),\n",
      "                               dtype='float32'),\n",
      "          trainable=True)\n",
      "      b_init = tf.zeros_initializer()\n",
      "      self.b = tf.Variable(\n",
      "          initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
      "          trainable=True)\n",
      "\n",
      "    def call(self, inputs):  # Defines the computation from inputs to outputs\n",
      "        return tf.matmul(inputs, self.w) + self.b\n",
      "\n",
      "  # Instantiates the layer.\n",
      "  linear_layer = SimpleDense(4)\n",
      "\n",
      "  # This will also call `build(input_shape)` and create the weights.\n",
      "  y = linear_layer(tf.ones((2, 2)))\n",
      "  assert len(linear_layer.weights) == 2\n",
      "\n",
      "  # These weights are trainable, so they're listed in `trainable_weights`:\n",
      "  assert len(linear_layer.trainable_weights) == 2\n",
      "  ```\n",
      "\n",
      "  Note that the method `add_weight()` offers a shortcut to create weights:\n",
      "\n",
      "  ```python\n",
      "  class SimpleDense(Layer):\n",
      "\n",
      "    def __init__(self, units=32):\n",
      "        super(SimpleDense, self).__init__()\n",
      "        self.units = units\n",
      "\n",
      "    def build(self, input_shape):\n",
      "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
      "                                 initializer='random_normal',\n",
      "                                 trainable=True)\n",
      "        self.b = self.add_weight(shape=(self.units,),\n",
      "                                 initializer='random_normal',\n",
      "                                 trainable=True)\n",
      "\n",
      "    def call(self, inputs):\n",
      "        return tf.matmul(inputs, self.w) + self.b\n",
      "  ```\n",
      "\n",
      "  Besides trainable weights, updated via backpropagation during training,\n",
      "  layers can also have non-trainable weights. These weights are meant to\n",
      "  be updated manually during `call()`. Here's a example layer that computes\n",
      "  the running sum of its inputs:\n",
      "\n",
      "  ```python\n",
      "  class ComputeSum(Layer):\n",
      "\n",
      "    def __init__(self, input_dim):\n",
      "        super(ComputeSum, self).__init__()\n",
      "        # Create a non-trainable weight.\n",
      "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
      "                                 trainable=False)\n",
      "\n",
      "    def call(self, inputs):\n",
      "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
      "        return self.total\n",
      "\n",
      "  my_sum = ComputeSum(2)\n",
      "  x = tf.ones((2, 2))\n",
      "\n",
      "  y = my_sum(x)\n",
      "  print(y.numpy())  # [2. 2.]\n",
      "\n",
      "  y = my_sum(x)\n",
      "  print(y.numpy())  # [4. 4.]\n",
      "\n",
      "  assert my_sum.weights == [my_sum.total]\n",
      "  assert my_sum.non_trainable_weights == [my_sum.total]\n",
      "  assert my_sum.trainable_weights == []\n",
      "  ```\n",
      "\n",
      "  For more information about creating layers, see the guide\n",
      "  [Writing custom layers and models with Keras](\n",
      "    https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
      "\n",
      "  About the layer's `dtype` attribute:\n",
      "\n",
      "  Each layer has a dtype, which is typically the dtype of the layer's\n",
      "  computations and variables. A layer's dtype can be queried via the\n",
      "  `Layer.dtype` property. The dtype is specified with the `dtype` constructor\n",
      "  argument. In TensorFlow 2, the dtype defaults to `tf.keras.backend.floatx()`\n",
      "  if no dtype is passed. `floatx()` itself defaults to \"float32\". Additionally,\n",
      "  layers will cast their inputs to the layer's dtype in TensorFlow 2. When mixed\n",
      "  precision is used, layers may have different computation and variable dtypes.\n",
      "  See `tf.keras.mixed_precision.experimental.Policy` for details on layer\n",
      "  dtypes.\n",
      "  \"\"\"\n",
      "\n",
      "  # See tf.Module for the usage of this property.\n",
      "  # The key for _obj_reference_counts_dict is a Trackable, which could be a\n",
      "  # variable or layer etc. tf.Module._flatten will fail to flatten the key\n",
      "  # since it is trying to convert Trackable to a string. This attribute can be\n",
      "  # ignored even after the fix of nest lib, since the trackable object should\n",
      "  # already been available as individual attributes. _obj_reference_counts_dict\n",
      "  # just contains a copy of them.\n",
      "  _TF_MODULE_IGNORED_PROPERTIES = frozenset(itertools.chain(\n",
      "      ('_obj_reference_counts_dict',),\n",
      "      module.Module._TF_MODULE_IGNORED_PROPERTIES\n",
      "  ))\n",
      "\n",
      "  # When loading from a SavedModel, Layers typically can be revived into a\n",
      "  # generic Layer wrapper. Sometimes, however, layers may implement methods\n",
      "  # that go beyond this wrapper, as in the case of PreprocessingLayers'\n",
      "  # `adapt` method. When this is the case, layer implementers can override\n",
      "  # must_restore_from_config to return True; layers with this property must\n",
      "  # be restored into their actual objects (and will fail if the object is\n",
      "  # not available to the restoration code).\n",
      "  _must_restore_from_config = False\n",
      "\n",
      "  @trackable.no_automatic_dependency_tracking\n",
      "  def __init__(self,\n",
      "               trainable=True,\n",
      "               name=None,\n",
      "               dtype=None,\n",
      "               dynamic=False,\n",
      "               **kwargs):\n",
      "    # These properties should be set by the user via keyword arguments.\n",
      "    # note that 'dtype', 'input_shape' and 'batch_input_shape'\n",
      "    # are only applicable to input layers: do not pass these keywords\n",
      "    # to non-input layers.\n",
      "    allowed_kwargs = {\n",
      "        'input_dim',\n",
      "        'input_shape',\n",
      "        'batch_input_shape',\n",
      "        'batch_size',\n",
      "        'weights',\n",
      "        'activity_regularizer',\n",
      "        'autocast',\n",
      "    }\n",
      "    # Validate optional keyword arguments.\n",
      "    generic_utils.validate_kwargs(kwargs, allowed_kwargs)\n",
      "\n",
      "    # Mutable properties\n",
      "    # Indicates whether the layer's weights are updated during training\n",
      "    # and whether the layer's updates are run during training.\n",
      "    self._trainable = trainable\n",
      "    # A stateful layer is a layer whose updates are run during inference too,\n",
      "    # for instance stateful RNNs.\n",
      "    self._stateful = False\n",
      "    # Indicates whether `build` needs to be called upon layer call, to create\n",
      "    # the layer's weights.\n",
      "    self.built = False\n",
      "    # Record the build input shape for loading purposes.\n",
      "    # TODO(kathywu): Move this to Layer._set_save_spec once cl/290121460 is\n",
      "    # submitted.\n",
      "    self._build_input_shape = None\n",
      "    self._saved_model_inputs_spec = None\n",
      "    # Provides information about which inputs are compatible with the layer.\n",
      "    self._input_spec = None\n",
      "\n",
      "    # `Layer.compute_mask` will be called at the end of `Layer.__call__` if\n",
      "    # `Layer.compute_mask` is overridden, or if the `Layer` subclass sets\n",
      "    # `self.supports_masking=True`.\n",
      "    self._supports_masking = not generic_utils.is_default(self.compute_mask)\n",
      "\n",
      "    self._init_set_name(name)\n",
      "    self._activity_regularizer = regularizers.get(\n",
      "        kwargs.pop('activity_regularizer', None))\n",
      "    self._maybe_create_attribute('_trainable_weights', [])\n",
      "    self._maybe_create_attribute('_non_trainable_weights', [])\n",
      "    self._updates = []\n",
      "    # Object to store all thread local layer properties.\n",
      "    self._thread_local = threading.local()\n",
      "    # A list of zero-argument lambdas which return Tensors, used for variable\n",
      "    # regularizers.\n",
      "    self._callable_losses = []\n",
      "    # A list of symbolic Tensors containing activity regularizers and losses\n",
      "    # manually added through `add_loss` in graph-building mode.\n",
      "    self._losses = []\n",
      "    # A list of metric instances corresponding to the symbolic metric tensors\n",
      "    # added using the `add_metric` API.\n",
      "    self._metrics = []\n",
      "    # Ensures the same metric is not added multiple times in `MirroredStrategy`.\n",
      "    self._metrics_lock = threading.Lock()\n",
      "\n",
      "    # Both graph and subclassed networks have a dtype policy. For graph\n",
      "    # networks, the policy's compute and variable dtypes are ignored, but other\n",
      "    # fields, like the loss scale, are used by Models. For subclassed networks,\n",
      "    # the compute and variable dtypes are used as like any ordinary layer.\n",
      "    self._set_dtype_policy(dtype)\n",
      "    # Boolean indicating whether the layer automatically casts its inputs to the\n",
      "    # layer's compute_dtype.\n",
      "    self._autocast = kwargs.get('autocast',\n",
      "                                base_layer_utils.v2_dtype_behavior_enabled())\n",
      "\n",
      "    # Dependencies tracked via attribute assignment.\n",
      "    # All layers in order of horizontal graph traversal.\n",
      "    # Entries are unique. For models includes input and output layers.\n",
      "    self._maybe_create_attribute('_layers', [])\n",
      "\n",
      "    # These lists will be filled via successive calls\n",
      "    # to self._add_inbound_node().\n",
      "    # Used in symbolic mode only, only in conjunction with graph-networks\n",
      "    self._inbound_nodes = []\n",
      "    self._outbound_nodes = []\n",
      "\n",
      "    self._init_call_fn_args()\n",
      "\n",
      "    # Whether the `call` method can be used to build a TF graph without issues.\n",
      "    # This attribute has no effect if the model is created using the Functional\n",
      "    # API. Instead, `model.dynamic` is determined based on the internal layers.\n",
      "    self._dynamic = dynamic\n",
      "\n",
      "    # Manage input shape information if passed.\n",
      "    if 'input_dim' in kwargs and 'input_shape' not in kwargs:\n",
      "      # Backwards compatibility: alias 'input_dim' to 'input_shape'.\n",
      "      kwargs['input_shape'] = (kwargs['input_dim'],)\n",
      "    if 'input_shape' in kwargs or 'batch_input_shape' in kwargs:\n",
      "      # In this case we will later create an input layer\n",
      "      # to insert before the current layer\n",
      "      if 'batch_input_shape' in kwargs:\n",
      "        batch_input_shape = tuple(kwargs['batch_input_shape'])\n",
      "      elif 'input_shape' in kwargs:\n",
      "        if 'batch_size' in kwargs:\n",
      "          batch_size = kwargs['batch_size']\n",
      "        else:\n",
      "          batch_size = None\n",
      "        batch_input_shape = (batch_size,) + tuple(kwargs['input_shape'])\n",
      "      self._batch_input_shape = batch_input_shape\n",
      "\n",
      "    # Manage initial weight values if passed.\n",
      "    self._initial_weights = kwargs.get('weights', None)\n",
      "\n",
      "    # Whether the layer will track any layers that is set as attribute on itself\n",
      "    # as sub-layers, the weights from the sub-layers will be included in the\n",
      "    # parent layer's variables() as well.\n",
      "    # Default to True, which means auto tracking is turned on. Certain subclass\n",
      "    # might want to turn it off, like Sequential model.\n",
      "    self._auto_track_sub_layers = True\n",
      "\n",
      "  @trackable.no_automatic_dependency_tracking\n",
      "  @generic_utils.default\n",
      "  def build(self, input_shape):\n",
      "    \"\"\"Creates the variables of the layer (optional, for subclass implementers).\n",
      "\n",
      "    This is a method that implementers of subclasses of `Layer` or `Model`\n",
      "    can override if they need a state-creation step in-between\n",
      "    layer instantiation and layer call.\n",
      "\n",
      "    This is typically used to create the weights of `Layer` subclasses.\n",
      "\n",
      "    Arguments:\n",
      "      input_shape: Instance of `TensorShape`, or list of instances of\n",
      "        `TensorShape` if the layer expects a list of inputs\n",
      "        (one instance per input).\n",
      "    \"\"\"\n",
      "    # Only record the build input shapes of overridden build methods.\n",
      "    if not hasattr(self.build, '_is_default'):\n",
      "      self._build_input_shape = input_shape\n",
      "    self.built = True\n",
      "\n",
      "  @doc_controls.for_subclass_implementers\n",
      "  def call(self, inputs, **kwargs):  # pylint: disable=unused-argument\n",
      "    \"\"\"This is where the layer's logic lives.\n",
      "\n",
      "    Note here that `call()` method in `tf.keras` is little bit different\n",
      "    from `keras` API. In `keras` API, you can pass support masking for\n",
      "    layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      "    method to support masking.\n",
      "\n",
      "    Arguments:\n",
      "        inputs: Input tensor, or list/tuple of input tensors.\n",
      "        **kwargs: Additional keyword arguments. Currently unused.\n",
      "\n",
      "    Returns:\n",
      "        A tensor or list/tuple of tensors.\n",
      "    \"\"\"\n",
      "    return inputs\n",
      "\n",
      "  @doc_controls.for_subclass_implementers\n",
      "  def _add_trackable(self, trackable_object, trainable):\n",
      "    \"\"\"Adds a Trackable object to this layer's state.\n",
      "\n",
      "    Arguments:\n",
      "      trackable_object: The tf.tracking.Trackable object to add.\n",
      "      trainable: Boolean, whether the variable should be part of the layer's\n",
      "        \"trainable_variables\" (e.g. variables, biases) or\n",
      "        \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      "\n",
      "    Returns:\n",
      "      The TrackableWeightHandler used to track this object.\n",
      "    \"\"\"\n",
      "    handler = base_layer_utils.TrackableWeightHandler(trackable_object)\n",
      "    if trainable:\n",
      "      self._trainable_weights.append(handler)\n",
      "    else:\n",
      "      self._non_trainable_weights.append(handler)\n",
      "    return handler\n",
      "\n",
      "  @doc_controls.for_subclass_implementers\n",
      "  def add_weight(self,\n",
      "                 name=None,\n",
      "                 shape=None,\n",
      "                 dtype=None,\n",
      "                 initializer=None,\n",
      "                 regularizer=None,\n",
      "                 trainable=None,\n",
      "                 constraint=None,\n",
      "                 partitioner=None,\n",
      "                 use_resource=None,\n",
      "                 synchronization=tf_variables.VariableSynchronization.AUTO,\n",
      "                 aggregation=tf_variables.VariableAggregation.NONE,\n",
      "                 **kwargs):\n",
      "    \"\"\"Adds a new variable to the layer.\n",
      "\n",
      "    Arguments:\n",
      "      name: Variable name.\n",
      "      shape: Variable shape. Defaults to scalar if unspecified.\n",
      "      dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      "      initializer: Initializer instance (callable).\n",
      "      regularizer: Regularizer instance (callable).\n",
      "      trainable: Boolean, whether the variable should be part of the layer's\n",
      "        \"trainable_variables\" (e.g. variables, biases)\n",
      "        or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      "        Note that `trainable` cannot be `True` if `synchronization`\n",
      "        is set to `ON_READ`.\n",
      "      constraint: Constraint instance (callable).\n",
      "      partitioner: Partitioner to be passed to the `Trackable` API.\n",
      "      use_resource: Whether to use `ResourceVariable`.\n",
      "      synchronization: Indicates when a distributed a variable will be\n",
      "        aggregated. Accepted values are constants defined in the class\n",
      "        `tf.VariableSynchronization`. By default the synchronization is set to\n",
      "        `AUTO` and the current `DistributionStrategy` chooses\n",
      "        when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      "        `trainable` must not be set to `True`.\n",
      "      aggregation: Indicates how a distributed variable will be aggregated.\n",
      "        Accepted values are constants defined in the class\n",
      "        `tf.VariableAggregation`.\n",
      "      **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      "        `collections`, `experimental_autocast` and `caching_device`.\n",
      "\n",
      "    Returns:\n",
      "      The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      "      instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      "      instance is returned.\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called with partitioned variable regularization and\n",
      "        eager execution is enabled.\n",
      "      ValueError: When giving unsupported dtype and no initializer or when\n",
      "        trainable has been set to True with synchronization set as `ON_READ`.\n",
      "    \"\"\"\n",
      "    if shape is None:\n",
      "      shape = ()\n",
      "    # Validate optional keyword arguments.\n",
      "    for kwarg in kwargs:\n",
      "      if kwarg not in ['getter', 'collections', 'experimental_autocast',\n",
      "                       'caching_device']:\n",
      "        raise TypeError('Unknown keyword argument:', kwarg)\n",
      "    getter = kwargs.pop('getter', base_layer_utils.make_variable)\n",
      "    collections_arg = kwargs.pop('collections', None)\n",
      "    # 'experimental_autocast' can be set to False by the caller to indicate an\n",
      "    # AutoCastVariable should never be created.\n",
      "    autocast = kwargs.pop('experimental_autocast', True)\n",
      "    # See the docstring for tf.Variable about the details for caching_device.\n",
      "    caching_device = kwargs.pop('caching_device', None)\n",
      "\n",
      "    if dtype is None:\n",
      "      dtype = self.dtype or backend.floatx()\n",
      "    dtype = dtypes.as_dtype(dtype)\n",
      "    if self._dtype_policy.variable_dtype is None:\n",
      "      # The policy is \"_infer\", so we infer the policy from the variable dtype.\n",
      "      self._set_dtype_policy(policy.Policy(dtype.base_dtype.name))\n",
      "    initializer = initializers.get(initializer)\n",
      "    regularizer = regularizers.get(regularizer)\n",
      "    constraint = constraints.get(constraint)\n",
      "\n",
      "    if synchronization == tf_variables.VariableSynchronization.ON_READ:\n",
      "      if trainable:\n",
      "        raise ValueError(\n",
      "            'Synchronization value can be set to '\n",
      "            'VariableSynchronization.ON_READ only for non-trainable variables. '\n",
      "            'You have specified trainable=True and '\n",
      "            'synchronization=VariableSynchronization.ON_READ.')\n",
      "      else:\n",
      "        # Set trainable to be false when variable is to be synced on read.\n",
      "        trainable = False\n",
      "    elif trainable is None:\n",
      "      trainable = True\n",
      "\n",
      "    # Initialize variable when no initializer provided\n",
      "    if initializer is None:\n",
      "      # If dtype is DT_FLOAT, provide a uniform unit scaling initializer\n",
      "      if dtype.is_floating:\n",
      "        initializer = initializers.get('glorot_uniform')\n",
      "      # If dtype is DT_INT/DT_UINT, provide a default value `zero`\n",
      "      # If dtype is DT_BOOL, provide a default value `FALSE`\n",
      "      elif dtype.is_integer or dtype.is_unsigned or dtype.is_bool:\n",
      "        initializer = initializers.get('zeros')\n",
      "      # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?\n",
      "      else:\n",
      "        raise ValueError('An initializer for variable %s of type %s is required'\n",
      "                         ' for layer %s' % (name, dtype.base_dtype, self.name))\n",
      "\n",
      "    if (autocast and self._dtype_policy.should_cast_variables and\n",
      "        dtype.is_floating):\n",
      "      # Wrap 'getter' with a version that returns an AutoCastVariable.\n",
      "      old_getter = getter\n",
      "      def getter(*args, **kwargs):  # pylint: disable=function-redefined\n",
      "        variable = old_getter(*args, **kwargs)\n",
      "        return autocast_variable.create_autocast_variable(variable)\n",
      "      # Also the caching_device does not work with the mixed precision API,\n",
      "      # disable it if it is specified.\n",
      "      # TODO(b/142020079): Reenable it once the bug is fixed.\n",
      "      if caching_device is not None:\n",
      "        tf_logging.warn('`caching_device` does not work with mixed precision '\n",
      "                        'API. Ignoring user specified `caching_device`.')\n",
      "        caching_device = None\n",
      "\n",
      "    variable = self._add_variable_with_custom_getter(\n",
      "        name=name,\n",
      "        shape=shape,\n",
      "        # TODO(allenl): a `make_variable` equivalent should be added as a\n",
      "        # `Trackable` method.\n",
      "        getter=getter,\n",
      "        # Manage errors in Layer rather than Trackable.\n",
      "        overwrite=True,\n",
      "        initializer=initializer,\n",
      "        dtype=dtype,\n",
      "        constraint=constraint,\n",
      "        trainable=trainable,\n",
      "        partitioner=partitioner,\n",
      "        use_resource=use_resource,\n",
      "        collections=collections_arg,\n",
      "        synchronization=synchronization,\n",
      "        aggregation=aggregation,\n",
      "        caching_device=caching_device)\n",
      "    if regularizer is not None:\n",
      "      # TODO(fchollet): in the future, this should be handled at the\n",
      "      # level of variable creation, and weight regularization losses\n",
      "      # should be variable attributes.\n",
      "      name_in_scope = variable.name[:variable.name.find(':')]\n",
      "      self._handle_weight_regularization(name_in_scope,\n",
      "                                         variable,\n",
      "                                         regularizer)\n",
      "    if isinstance(\n",
      "        variable,\n",
      "        (tf_variables.PartitionedVariable, sharded_variable.ShardedVariable)):\n",
      "      for v in variable:\n",
      "        backend.track_variable(v)\n",
      "        if trainable:\n",
      "          self._trainable_weights.append(v)\n",
      "        else:\n",
      "          self._non_trainable_weights.append(v)\n",
      "    else:\n",
      "      backend.track_variable(variable)\n",
      "      if trainable:\n",
      "        self._trainable_weights.append(variable)\n",
      "      else:\n",
      "        self._non_trainable_weights.append(variable)\n",
      "    return variable\n",
      "\n",
      "  @generic_utils.default\n",
      "  def get_config(self):\n",
      "    \"\"\"Returns the config of the layer.\n",
      "\n",
      "    A layer config is a Python dictionary (serializable)\n",
      "    containing the configuration of a layer.\n",
      "    The same layer can be reinstantiated later\n",
      "    (without its trained weights) from this configuration.\n",
      "\n",
      "    The config of a layer does not include connectivity\n",
      "    information, nor the layer class name. These are handled\n",
      "    by `Network` (one layer of abstraction above).\n",
      "\n",
      "    Returns:\n",
      "        Python dictionary.\n",
      "    \"\"\"\n",
      "    all_args = tf_inspect.getfullargspec(self.__init__).args\n",
      "    config = {\n",
      "        'name': self.name,\n",
      "        'trainable': self.trainable,\n",
      "    }\n",
      "    if hasattr(self, '_batch_input_shape'):\n",
      "      config['batch_input_shape'] = self._batch_input_shape\n",
      "    config['dtype'] = policy.serialize(self._dtype_policy)\n",
      "    if hasattr(self, 'dynamic'):\n",
      "      # Only include `dynamic` in the `config` if it is `True`\n",
      "      if self.dynamic:\n",
      "        config['dynamic'] = self.dynamic\n",
      "      elif 'dynamic' in all_args:\n",
      "        all_args.remove('dynamic')\n",
      "    expected_args = config.keys()\n",
      "    # Finds all arguments in the `__init__` that are not in the config:\n",
      "    extra_args = [arg for arg in all_args if arg not in expected_args]\n",
      "    # Check that either the only argument in the `__init__` is  `self`,\n",
      "    # or that `get_config` has been overridden:\n",
      "    if len(extra_args) > 1 and hasattr(self.get_config, '_is_default'):\n",
      "      raise NotImplementedError('Layer %s has arguments in `__init__` and '\n",
      "                                'therefore must override `get_config`.' %\n",
      "                                self.__class__.__name__)\n",
      "    return config\n",
      "\n",
      "  @classmethod\n",
      "  def from_config(cls, config):\n",
      "    \"\"\"Creates a layer from its config.\n",
      "\n",
      "    This method is the reverse of `get_config`,\n",
      "    capable of instantiating the same layer from the config\n",
      "    dictionary. It does not handle layer connectivity\n",
      "    (handled by Network), nor weights (handled by `set_weights`).\n",
      "\n",
      "    Arguments:\n",
      "        config: A Python dictionary, typically the\n",
      "            output of get_config.\n",
      "\n",
      "    Returns:\n",
      "        A layer instance.\n",
      "    \"\"\"\n",
      "    return cls(**config)\n",
      "\n",
      "  def compute_output_shape(self, input_shape):\n",
      "    \"\"\"Computes the output shape of the layer.\n",
      "\n",
      "    If the layer has not been built, this method will call `build` on the\n",
      "    layer. This assumes that the layer will later be used with inputs that\n",
      "    match the input shape provided here.\n",
      "\n",
      "    Arguments:\n",
      "        input_shape: Shape tuple (tuple of integers)\n",
      "            or list of shape tuples (one per output tensor of the layer).\n",
      "            Shape tuples can include None for free dimensions,\n",
      "            instead of an integer.\n",
      "\n",
      "    Returns:\n",
      "        An input shape tuple.\n",
      "    \"\"\"\n",
      "    if context.executing_eagerly():\n",
      "      # In this case we build the model first in order to do shape inference.\n",
      "      # This is acceptable because the framework only calls\n",
      "      # `compute_output_shape` on shape values that the layer would later be\n",
      "      # built for. It would however cause issues in case a user attempts to\n",
      "      # use `compute_output_shape` manually with shapes that are incompatible\n",
      "      # with the shape the Layer will be called on (these users will have to\n",
      "      # implement `compute_output_shape` themselves).\n",
      "      self._maybe_build(input_shape)\n",
      "      with func_graph.FuncGraph(str(self.name) + '_scratch_graph').as_default():\n",
      "        input_shape = tf_utils.convert_shapes(input_shape, to_tuples=False)\n",
      "        def _make_placeholder_like(shape):\n",
      "          ph = backend.placeholder(shape=shape, dtype=self.dtype)\n",
      "          ph._keras_mask = None\n",
      "          return ph\n",
      "        inputs = nest.map_structure(_make_placeholder_like, input_shape)\n",
      "        try:\n",
      "          outputs = self(inputs, training=False)\n",
      "        except TypeError as e:\n",
      "          six.raise_from(\n",
      "              NotImplementedError(\n",
      "                  'We could not automatically infer the static shape of the '\n",
      "                  'layer\\'s output. Please implement the '\n",
      "                  '`compute_output_shape` method on your layer (%s).' %\n",
      "                  self.__class__.__name__), e)\n",
      "      return nest.map_structure(lambda t: t.shape, outputs)\n",
      "    raise NotImplementedError\n",
      "\n",
      "  @doc_controls.for_subclass_implementers\n",
      "  def compute_output_signature(self, input_signature):\n",
      "    \"\"\"Compute the output tensor signature of the layer based on the inputs.\n",
      "\n",
      "    Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      "    and dtype information for a tensor. This method allows layers to provide\n",
      "    output dtype information if it is different from the input dtype.\n",
      "    For any layer that doesn't implement this function,\n",
      "    the framework will fall back to use `compute_output_shape`, and will\n",
      "    assume that the output dtype matches the input dtype.\n",
      "\n",
      "    Args:\n",
      "      input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      "        objects, describing a candidate input for the layer.\n",
      "\n",
      "    Returns:\n",
      "      Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      "        how the layer would transform the provided input.\n",
      "\n",
      "    Raises:\n",
      "      TypeError: If input_signature contains a non-TensorSpec object.\n",
      "    \"\"\"\n",
      "    def check_type_return_shape(s):\n",
      "      if not isinstance(s, tensor_spec.TensorSpec):\n",
      "        raise TypeError(\n",
      "            'Only TensorSpec signature types are supported, '\n",
      "            'but saw signature signature entry: {}.'.format(s))\n",
      "      return s.shape\n",
      "    input_shape = nest.map_structure(check_type_return_shape, input_signature)\n",
      "    output_shape = self.compute_output_shape(input_shape)\n",
      "    dtype = self._compute_dtype\n",
      "    if dtype is None:\n",
      "      input_dtypes = [s.dtype for s in nest.flatten(input_signature)]\n",
      "      # Default behavior when self.dtype is None, is to use the first input's\n",
      "      # dtype.\n",
      "      dtype = input_dtypes[0]\n",
      "    return nest.map_structure(\n",
      "        lambda s: tensor_spec.TensorSpec(dtype=dtype, shape=s),\n",
      "        output_shape)\n",
      "\n",
      "  def _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs):\n",
      "    if self.dynamic:\n",
      "      # We will use static shape inference to return symbolic tensors\n",
      "      # matching the specifications of the layer outputs.\n",
      "      # Since `self.dynamic` is True, we will never attempt to\n",
      "      # run the underlying TF graph (which is disconnected).\n",
      "      # TODO(fchollet): consider py_func as an alternative, which\n",
      "      # would enable us to run the underlying graph if needed.\n",
      "      input_signature = nest.map_structure(\n",
      "          lambda x: tensor_spec.TensorSpec(shape=x.shape, dtype=x.dtype),\n",
      "          inputs)\n",
      "      output_signature = self.compute_output_signature(input_signature)\n",
      "      return nest.map_structure(keras_tensor.KerasTensor, output_signature)\n",
      "    else:\n",
      "      return self._infer_output_signature(inputs, args, kwargs, input_masks)\n",
      "\n",
      "  def _infer_output_signature(self, inputs, args, kwargs, input_masks):\n",
      "    \"\"\"TODO(kaftan): Docstring.\"\"\"\n",
      "\n",
      "    call_fn = self.call\n",
      "    # Wrapping `call` function in autograph to allow for dynamic control\n",
      "    # flow and control dependencies in call. We are limiting this to\n",
      "    # subclassed layers as autograph is strictly needed only for\n",
      "    # subclassed layers and models.\n",
      "    # tf_convert will respect the value of autograph setting in the\n",
      "    # enclosing tf.function, if any.\n",
      "    if (base_layer_utils.is_subclassed(self) and\n",
      "        not base_layer_utils.from_saved_model(self)):\n",
      "      call_fn = autograph.tf_convert(self.call, ag_ctx.control_status_ctx())\n",
      "\n",
      "    # We enter a scratch graph and build placeholder inputs inside of it that\n",
      "    # match the input args.\n",
      "    # We then call the layer inside of the scratch graph to identify the\n",
      "    # output signatures, then we build KerasTensors corresponding to those\n",
      "    # outputs.\n",
      "    scratch_graph = func_graph.FuncGraph(str(self.name) + '_scratch_graph')\n",
      "    with scratch_graph.as_default():\n",
      "      inputs = nest.map_structure(\n",
      "          keras_tensor.keras_tensor_to_placeholder, inputs)\n",
      "      args = nest.map_structure(\n",
      "          keras_tensor.keras_tensor_to_placeholder, args)\n",
      "      kwargs = nest.map_structure(\n",
      "          keras_tensor.keras_tensor_to_placeholder, kwargs)\n",
      "      input_masks = nest.map_structure(\n",
      "          keras_tensor.keras_tensor_to_placeholder, input_masks)\n",
      "\n",
      "      inputs = self._maybe_cast_inputs(inputs)\n",
      "\n",
      "      # try:\n",
      "      with backend.name_scope(self._name_scope()):\n",
      "        with ops.enable_auto_cast_variables(self._compute_dtype_object):\n",
      "          # Build layer if applicable (if the `build` method has been\n",
      "          # overridden).\n",
      "          # TODO(kaftan): do we maybe_build here, or have we already done it?\n",
      "          self._maybe_build(inputs)\n",
      "          outputs = call_fn(inputs, *args, **kwargs)\n",
      "\n",
      "        self._handle_activity_regularization(inputs, outputs)\n",
      "      self._set_mask_metadata(inputs, outputs, input_masks,\n",
      "                              build_graph=False)\n",
      "\n",
      "    outputs = nest.map_structure(keras_tensor.keras_tensor_from_tensor, outputs)\n",
      "    if hasattr(self, '_set_inputs') and not self.inputs:\n",
      "      # TODO(kaftan): figure out if we ned to do this at all\n",
      "      # Subclassed network: explicitly set metadata normally set by\n",
      "      # a call to self._set_inputs().\n",
      "      self._set_inputs(inputs, outputs)\n",
      "    del scratch_graph\n",
      "    return outputs\n",
      "\n",
      "  @generic_utils.default\n",
      "  def compute_mask(self, inputs, mask=None):  # pylint: disable=unused-argument\n",
      "    \"\"\"Computes an output mask tensor.\n",
      "\n",
      "    Arguments:\n",
      "        inputs: Tensor or list of tensors.\n",
      "        mask: Tensor or list of tensors.\n",
      "\n",
      "    Returns:\n",
      "        None or a tensor (or list of tensors,\n",
      "            one per output tensor of the layer).\n",
      "    \"\"\"\n",
      "    if not self._supports_masking:\n",
      "      if any(m is not None for m in nest.flatten(mask)):\n",
      "        raise TypeError('Layer ' + self.name + ' does not support masking, '\n",
      "                        'but was passed an input_mask: ' + str(mask))\n",
      "      # masking not explicitly supported: return None as mask.\n",
      "      return None\n",
      "    # if masking is explicitly supported, by default\n",
      "    # carry over the input mask\n",
      "    return mask\n",
      "\n",
      "  def __call__(self, *args, **kwargs):\n",
      "    \"\"\"Wraps `call`, applying pre- and post-processing steps.\n",
      "\n",
      "    Arguments:\n",
      "      *args: Positional arguments to be passed to `self.call`.\n",
      "      **kwargs: Keyword arguments to be passed to `self.call`.\n",
      "\n",
      "    Returns:\n",
      "      Output tensor(s).\n",
      "\n",
      "    Note:\n",
      "      - The following optional keyword arguments are reserved for specific uses:\n",
      "        * `training`: Boolean scalar tensor of Python boolean indicating\n",
      "          whether the `call` is meant for training or inference.\n",
      "        * `mask`: Boolean input mask.\n",
      "      - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      "        layers do), its default value will be set to the mask generated\n",
      "        for `inputs` by the previous layer (if `input` did come from\n",
      "        a layer that generated a corresponding mask, i.e. if it came from\n",
      "        a Keras layer with masking support.\n",
      "\n",
      "    Raises:\n",
      "      ValueError: if the layer's `call` method returns None (an invalid value).\n",
      "      RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      "    \"\"\"\n",
      "    if not hasattr(self, '_thread_local'):\n",
      "      raise RuntimeError(\n",
      "          'You must call `super().__init__()` in the layer constructor.')\n",
      "\n",
      "    # `inputs` (the first arg in the method spec) is special cased in\n",
      "    # layer call due to historical reasons.\n",
      "    # This special casing currently takes the form of:\n",
      "    # - 'inputs' must be explicitly passed. A layer cannot have zero arguments,\n",
      "    #   and inputs cannot have been provided via the default value of a kwarg.\n",
      "    # - numpy/scalar values in `inputs` get converted to tensors\n",
      "    # - implicit masks / mask metadata are only collected from 'inputs`\n",
      "    # - Layers are built using shape info from 'inputs' only\n",
      "    # - input_spec compatibility is only checked against `inputs`\n",
      "    # - mixed precision casting (autocast) is only applied to `inputs`,\n",
      "    #   not to any other argument.\n",
      "    # - setting the SavedModel saving spec.\n",
      "    inputs, args, kwargs = self._split_out_first_arg(args, kwargs)\n",
      "    input_list = nest.flatten(inputs)\n",
      "\n",
      "    # Functional Model construction mode is invoked when `Layer`s are called on\n",
      "    # symbolic `KerasTensor`s, i.e.:\n",
      "    # >> inputs = tf.keras.Input(10)\n",
      "    # >> outputs = MyLayer()(inputs)  # Functional construction mode.\n",
      "    # >> model = tf.keras.Model(inputs, outputs)\n",
      "    if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\n",
      "      return self._functional_construction_call(inputs, args, kwargs,\n",
      "                                                input_list)\n",
      "\n",
      "    # Maintains info about the `Layer.call` stack.\n",
      "    call_context = base_layer_utils.call_context()\n",
      "\n",
      "    # Accept NumPy and scalar inputs by converting to Tensors.\n",
      "    if any(isinstance(x, (np.ndarray, float, int)) for x in input_list):\n",
      "      inputs = nest.map_structure(_convert_numpy_or_python_types, inputs)\n",
      "      input_list = nest.flatten(inputs)\n",
      "\n",
      "    # Handle `mask` propagation from previous layer to current layer. Masks can\n",
      "    # be propagated explicitly via the `mask` argument, or implicitly via\n",
      "    # setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed\n",
      "    # explicitly take priority.\n",
      "    input_masks, mask_is_implicit = self._get_input_masks(\n",
      "        inputs, input_list, args, kwargs)\n",
      "    if self._expects_mask_arg and mask_is_implicit:\n",
      "      kwargs['mask'] = input_masks\n",
      "\n",
      "    # Training mode for `Layer.call` is set via (in order of priority):\n",
      "    # (1) The `training` argument passed to this `Layer.call`, if it is not None\n",
      "    # (2) The training mode of an outer `Layer.call`.\n",
      "    # (3) The default mode set by `tf.keras.backend.set_learning_phase` (if set)\n",
      "    # (4) Any non-None default value for `training` specified in the call\n",
      "    #  signature\n",
      "    # (5) False (treating the layer as if it's in inference)\n",
      "    args, kwargs, training_mode = self._set_training_mode(\n",
      "        args, kwargs, call_context)\n",
      "\n",
      "    # Losses are cleared for all sublayers on the outermost `Layer.call`.\n",
      "    # Losses are not cleared on inner `Layer.call`s, because sublayers can be\n",
      "    # called multiple times.\n",
      "    if not call_context.in_call:\n",
      "      self._clear_losses()\n",
      "\n",
      "    eager = context.executing_eagerly()\n",
      "    with call_context.enter(\n",
      "        layer=self,\n",
      "        inputs=inputs,\n",
      "        build_graph=not eager,\n",
      "        training=training_mode):\n",
      "\n",
      "      if self._autocast:\n",
      "        inputs = self._maybe_cast_inputs(inputs, input_list)\n",
      "\n",
      "      if eager:\n",
      "        call_fn = self.call\n",
      "        name_scope = self._name\n",
      "      else:\n",
      "        input_spec.assert_input_compatibility(self.input_spec, inputs,\n",
      "                                              self.name)\n",
      "        name_scope = self._name_scope()  # Avoid autoincrementing.\n",
      "        call_fn = self._autographed_call()\n",
      "\n",
      "      with ops.name_scope_v2(name_scope):\n",
      "        if not self.built:\n",
      "          self._maybe_build(inputs)\n",
      "\n",
      "        with ops.enable_auto_cast_variables(self._compute_dtype_object):\n",
      "          outputs = call_fn(inputs, *args, **kwargs)\n",
      "\n",
      "        if self._activity_regularizer:\n",
      "          self._handle_activity_regularization(inputs, outputs)\n",
      "        if self._supports_masking:\n",
      "          self._set_mask_metadata(inputs, outputs, input_masks, not eager)\n",
      "        if self._saved_model_inputs_spec is None:\n",
      "          self._set_save_spec(inputs)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "  def _functional_construction_call(self, inputs, args, kwargs, input_list):\n",
      "    call_context = base_layer_utils.call_context()\n",
      "\n",
      "    # Accept NumPy and scalar inputs by converting to Tensors.\n",
      "    if any(isinstance(x, (np.ndarray, float, int)) for x in input_list):\n",
      "\n",
      "      def _convert_non_tensor(x):\n",
      "        # Don't call `ops.convert_to_tensor_v2` on all `inputs` because\n",
      "        # `SparseTensors` can't be converted to `Tensor`.\n",
      "        if isinstance(x, (np.ndarray, float, int)):\n",
      "          return ops.convert_to_tensor_v2(x)\n",
      "        return x\n",
      "\n",
      "      inputs = nest.map_structure(_convert_non_tensor, inputs)\n",
      "      input_list = nest.flatten(inputs)\n",
      "\n",
      "    # Handle `mask` propagation from previous layer to current layer. Masks can\n",
      "    # be propagated explicitly via the `mask` argument, or implicitly via\n",
      "    # setting the `_keras_mask` attribute on the inputs to a Layer. Masks passed\n",
      "    # explicitly take priority.\n",
      "    mask_arg_passed_by_framework = False\n",
      "    input_masks, mask_is_implicit = self._get_input_masks(\n",
      "        inputs, input_list, args, kwargs)\n",
      "    if self._expects_mask_arg and mask_is_implicit:\n",
      "      kwargs['mask'] = input_masks\n",
      "      mask_arg_passed_by_framework = True\n",
      "\n",
      "    # If `training` argument is None or not explicitly passed,\n",
      "    # propagate `training` value from this layer's calling layer.\n",
      "    training_value = None\n",
      "    training_arg_passed_by_framework = False\n",
      "    # Priority 1: `training` was explicitly passed a non-None value.\n",
      "    if self._call_arg_was_passed('training', args, kwargs):\n",
      "      training_value = self._get_call_arg_value('training', args, kwargs)\n",
      "      if not self._expects_training_arg:\n",
      "        kwargs.pop('training')\n",
      "\n",
      "    if training_value is None:\n",
      "      # Priority 2: `training` was passed to a parent layer.\n",
      "      if call_context.training is not None:\n",
      "        training_value = call_context.training\n",
      "      # Priority 3: `learning_phase()` has been set.\n",
      "      elif backend.global_learning_phase_is_set():\n",
      "        training_value = backend.learning_phase()\n",
      "        # Force the training_value to be bool type which matches to the contract\n",
      "        # for layer/model call args.\n",
      "        if tensor_util.is_tensor(training_value):\n",
      "          training_value = math_ops.cast(training_value, dtypes.bool)\n",
      "        else:\n",
      "          training_value = bool(training_value)\n",
      "      # Priority 4: trace layer with the default training argument specified\n",
      "      # in the `call` signature (or in inference mode if the `call` signature\n",
      "      # specifies no non-None default).\n",
      "      else:\n",
      "        training_value = self._default_training_arg\n",
      "      # In cases (2), (3), (4) the training argument is passed automatically\n",
      "      # by the framework, and will not be hard-coded into the model.\n",
      "      if self._expects_training_arg:\n",
      "        args, kwargs = self._set_call_arg_value('training', training_value,\n",
      "                                                args, kwargs)\n",
      "        training_arg_passed_by_framework = True\n",
      "\n",
      "    if keras_tensor.keras_tensors_enabled():\n",
      "      with call_context.enter(\n",
      "          layer=self, inputs=inputs, build_graph=True, training=training_value):\n",
      "        # Check input assumptions set after layer building, e.g. input shape.\n",
      "        outputs = self._keras_tensor_symbolic_call(\n",
      "            inputs, input_masks, args, kwargs)\n",
      "\n",
      "        if outputs is None:\n",
      "          raise ValueError('A layer\\'s `call` method should return a '\n",
      "                           'Tensor or a list of Tensors, not None '\n",
      "                           '(layer: ' + self.name + ').')\n",
      "        if training_arg_passed_by_framework:\n",
      "          args, kwargs = self._set_call_arg_value(\n",
      "              'training', None, args, kwargs, pop_kwarg_if_none=True)\n",
      "        if mask_arg_passed_by_framework:\n",
      "          kwargs.pop('mask')\n",
      "        # Node connectivity does not special-case the first argument.\n",
      "        outputs = self._set_connectivity_metadata((inputs,) + args, kwargs,\n",
      "                                                  outputs)\n",
      "        return outputs\n",
      "\n",
      "    # Only create Keras history if at least one tensor originates from a\n",
      "    # `keras.Input`. Otherwise this Layer may be being used outside the Keras\n",
      "    # framework.\n",
      "    # TODO(kaftan): make this not special case inputs\n",
      "    if base_layer_utils.needs_keras_history(inputs):\n",
      "      base_layer_utils.create_keras_history(inputs)\n",
      "\n",
      "    with call_context.enter(\n",
      "        layer=self, inputs=inputs, build_graph=True, training=training_value):\n",
      "      # Symbolic execution on symbolic tensors. We will attempt to build\n",
      "      # the corresponding TF subgraph inside `backend.get_graph()`\n",
      "      # TODO(reedwm): We should assert input compatibility after the inputs\n",
      "      # are casted, not before.\n",
      "      input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n",
      "      graph = backend.get_graph()\n",
      "      # Use `self._name_scope()` to avoid auto-incrementing the name.\n",
      "      with graph.as_default(), backend.name_scope(self._name_scope()):\n",
      "        # Build layer if applicable (if the `build` method has been\n",
      "        # overridden).\n",
      "        self._maybe_build(inputs)\n",
      "        cast_inputs = self._maybe_cast_inputs(inputs, input_list)\n",
      "\n",
      "        if not self.dynamic:\n",
      "          # Wrapping `call` function in autograph to allow for dynamic control\n",
      "          # flow and control dependencies in call. We are limiting this to\n",
      "          # subclassed layers as autograph is strictly needed only for\n",
      "          # subclassed layers and models.\n",
      "          # tf_convert will respect the value of autograph setting in the\n",
      "          # enclosing tf.function, if any.\n",
      "          if (base_layer_utils.is_subclassed(self) and\n",
      "              not base_layer_utils.from_saved_model(self)):\n",
      "            call_fn = autograph.tf_convert(self.call,\n",
      "                                           ag_ctx.control_status_ctx())\n",
      "          else:\n",
      "            call_fn = self.call\n",
      "\n",
      "          try:\n",
      "            with ops.enable_auto_cast_variables(self._compute_dtype_object):\n",
      "              outputs = call_fn(cast_inputs, *args, **kwargs)\n",
      "\n",
      "          except errors.OperatorNotAllowedInGraphError as e:\n",
      "            raise TypeError('You are attempting to use Python control '\n",
      "                            'flow in a layer that was not declared to be '\n",
      "                            'dynamic. Pass `dynamic=True` to the class '\n",
      "                            'constructor.\\nEncountered error:\\n\"\"\"\\n' + str(e) +\n",
      "                            '\\n\"\"\"')\n",
      "        else:\n",
      "          # We will use static shape inference to return symbolic tensors\n",
      "          # matching the specifications of the layer outputs.\n",
      "          # Since `self.dynamic` is True, we will never attempt to\n",
      "          # run the underlying TF graph (which is disconnected).\n",
      "          # TODO(fchollet): consider py_func as an alternative, which\n",
      "          # would enable us to run the underlying graph if needed.\n",
      "          outputs = self._symbolic_call(inputs)\n",
      "\n",
      "        if outputs is None:\n",
      "          raise ValueError('A layer\\'s `call` method should return a '\n",
      "                           'Tensor or a list of Tensors, not None '\n",
      "                           '(layer: ' + self.name + ').')\n",
      "        # TODO(kaftan): This should be 'any' and check all args\n",
      "        if base_layer_utils.have_all_keras_metadata(inputs):\n",
      "          if training_arg_passed_by_framework:\n",
      "            args, kwargs = self._set_call_arg_value(\n",
      "                'training', None, args, kwargs, pop_kwarg_if_none=True)\n",
      "          if mask_arg_passed_by_framework:\n",
      "            kwargs.pop('mask')\n",
      "          # Node connectivity does not special-case the first argument.\n",
      "          outputs = self._set_connectivity_metadata((inputs,) + args, kwargs,\n",
      "                                                    outputs)\n",
      "        self._handle_activity_regularization(inputs, outputs)\n",
      "        self._set_mask_metadata(inputs, outputs, input_masks, True)\n",
      "        if hasattr(self, '_set_inputs') and not self.inputs:\n",
      "          # Subclassed network: explicitly set metadata normally set by\n",
      "          # a call to self._set_inputs().\n",
      "          self._set_inputs(cast_inputs, outputs)\n",
      "\n",
      "    return outputs\n",
      "\n",
      "  def _set_training_mode(self, args, kwargs, call_context):\n",
      "    training_mode = None\n",
      "    if self._expects_training_arg:\n",
      "      # (1) `training` was passed to this `Layer.call`.\n",
      "      if self._call_arg_was_passed('training', args, kwargs):\n",
      "        training_mode = self._get_call_arg_value('training', args, kwargs)\n",
      "      # If no `training` arg was passed, or `None` was explicitly passed,\n",
      "      # the framework will make a decision about the training mode is.\n",
      "      if training_mode is None:\n",
      "        call_ctx_training = call_context.training\n",
      "        # (2) `training` mode is inferred from an outer `Layer.call`.\n",
      "        if call_ctx_training is not None:\n",
      "          training_mode = call_ctx_training\n",
      "        # (3) User set `tf.keras.backend.set_learning_phase`.\n",
      "        elif backend.global_learning_phase_is_set():\n",
      "          training_mode = backend.learning_phase()\n",
      "          # Ensure value is a `bool` or `tf.bool`.\n",
      "          if isinstance(training_mode, bool):\n",
      "            pass\n",
      "          elif tensor_util.is_tensor(training_mode):\n",
      "            training_mode = math_ops.cast(training_mode, dtypes.bool)\n",
      "          else:\n",
      "            training_mode = bool(training_mode)\n",
      "        # (4) We default to using `call`'s default value for `training`,\n",
      "        # or treating the layer as if it is in inference if no non-None default\n",
      "        # is specified in the `call` signature.\n",
      "        else:\n",
      "          training_mode = self._default_training_arg\n",
      "\n",
      "        # For case (2), (3), (4) `training` arg is passed by framework.\n",
      "        args, kwargs = self._set_call_arg_value('training', training_mode, args,\n",
      "                                                kwargs)\n",
      "    else:\n",
      "      if 'training' in kwargs:\n",
      "        # `training` was passed to this `Layer` but is not needed for\n",
      "        # `Layer.call`. It will set the default mode for inner `Layer.call`s.\n",
      "        training_mode = kwargs.pop('training')\n",
      "      else:\n",
      "        # Grab the current `training` mode from any outer `Layer.call`.\n",
      "        training_mode = call_context.training\n",
      "\n",
      "    return args, kwargs, training_mode\n",
      "\n",
      "  def _autographed_call(self):\n",
      "    # Wrapping `call` function in autograph to allow for dynamic control\n",
      "    # flow and control dependencies in call. We are limiting this to\n",
      "    # subclassed layers as autograph is strictly needed only for\n",
      "    # subclassed layers and models.\n",
      "    # tf_convert will respect the value of autograph setting in the\n",
      "    # enclosing tf.function, if any.\n",
      "    if (base_layer_utils.is_subclassed(self) and\n",
      "        not base_layer_utils.from_saved_model(self)):\n",
      "      return autograph.tf_convert(self.call, ag_ctx.control_status_ctx())\n",
      "    else:\n",
      "      return self.call\n",
      "\n",
      "  @property\n",
      "  def dtype(self):\n",
      "    \"\"\"Dtype used by the weights of the layer, set in the constructor.\"\"\"\n",
      "    return self._dtype_policy.variable_dtype\n",
      "\n",
      "  @property\n",
      "  def name(self):\n",
      "    \"\"\"Name of the layer (string), set in the constructor.\"\"\"\n",
      "    return self._name\n",
      "\n",
      "  @property\n",
      "  def supports_masking(self):\n",
      "    \"\"\"Whether this layer supports computing a mask using `compute_mask`.\"\"\"\n",
      "    return self._supports_masking\n",
      "\n",
      "  @supports_masking.setter\n",
      "  def supports_masking(self, value):\n",
      "    self._supports_masking = value\n",
      "\n",
      "  @property\n",
      "  def dynamic(self):\n",
      "    \"\"\"Whether the layer is dynamic (eager-only); set in the constructor.\"\"\"\n",
      "    return any(layer._dynamic for layer in self._flatten_layers())\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def stateful(self):\n",
      "    return any(layer._stateful for layer in self._flatten_layers())\n",
      "\n",
      "  @stateful.setter\n",
      "  def stateful(self, value):\n",
      "    self._stateful = value\n",
      "\n",
      "  @property\n",
      "  def trainable(self):\n",
      "    return self._trainable\n",
      "\n",
      "  @trainable.setter\n",
      "  def trainable(self, value):\n",
      "    for layer in self._flatten_layers():\n",
      "      layer._trainable = value\n",
      "\n",
      "  @property\n",
      "  def activity_regularizer(self):\n",
      "    \"\"\"Optional regularizer function for the output of this layer.\"\"\"\n",
      "    return self._activity_regularizer\n",
      "\n",
      "  @activity_regularizer.setter\n",
      "  def activity_regularizer(self, regularizer):\n",
      "    \"\"\"Optional regularizer function for the output of this layer.\"\"\"\n",
      "    self._activity_regularizer = regularizer\n",
      "\n",
      "  @property\n",
      "  def input_spec(self):\n",
      "    \"\"\"`InputSpec` instance(s) describing the input format for this layer.\n",
      "\n",
      "    When you create a layer subclass, you can set `self.input_spec` to enable\n",
      "    the layer to run input compatibility checks when it is called.\n",
      "    Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      "    of rank 4. As such, you can set, in `__init__()`:\n",
      "\n",
      "    ```python\n",
      "    self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      "    ```\n",
      "\n",
      "    Now, if you try to call the layer on an input that isn't rank 4\n",
      "    (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      "    error:\n",
      "\n",
      "    ```\n",
      "    ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      "    expected ndim=4, found ndim=1. Full shape received: [2]\n",
      "    ```\n",
      "\n",
      "    Input checks that can be specified via `input_spec` include:\n",
      "    - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      "    - Shape\n",
      "    - Rank (ndim)\n",
      "    - Dtype\n",
      "\n",
      "    For more information, see `tf.keras.layers.InputSpec`.\n",
      "\n",
      "    Returns:\n",
      "      A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      "    \"\"\"\n",
      "    return self._input_spec\n",
      "\n",
      "  @input_spec.setter\n",
      "  # Must be decorated to prevent tracking, since the input_spec can be nested\n",
      "  # InputSpec objects.\n",
      "  @trackable.no_automatic_dependency_tracking\n",
      "  def input_spec(self, value):\n",
      "    for v in nest.flatten(value):\n",
      "      if v is not None and not isinstance(v, InputSpec):\n",
      "        raise TypeError('Layer input_spec must be an instance of InputSpec. '\n",
      "                        'Got: {}'.format(v))\n",
      "    self._input_spec = value\n",
      "\n",
      "  @property\n",
      "  def trainable_weights(self):\n",
      "    \"\"\"List of all trainable weights tracked by this layer.\n",
      "\n",
      "    Trainable weights are updated via gradient descent during training.\n",
      "\n",
      "    Returns:\n",
      "      A list of trainable variables.\n",
      "    \"\"\"\n",
      "    if self.trainable:\n",
      "      children_weights = self._gather_children_attribute('trainable_weights')\n",
      "      return self._dedup_weights(self._trainable_weights + children_weights)\n",
      "    else:\n",
      "      return []\n",
      "\n",
      "  @property\n",
      "  def non_trainable_weights(self):\n",
      "    \"\"\"List of all non-trainable weights tracked by this layer.\n",
      "\n",
      "    Non-trainable weights are *not* updated during training. They are expected\n",
      "    to be updated manually in `call()`.\n",
      "\n",
      "    Returns:\n",
      "      A list of non-trainable variables.\n",
      "    \"\"\"\n",
      "    if self.trainable:\n",
      "      children_weights = self._gather_children_attribute(\n",
      "          'non_trainable_weights')\n",
      "      non_trainable_weights = self._non_trainable_weights + children_weights\n",
      "    else:\n",
      "      children_weights = self._gather_children_attribute('weights')\n",
      "      non_trainable_weights = (\n",
      "          self._trainable_weights + self._non_trainable_weights +\n",
      "          children_weights)\n",
      "    return self._dedup_weights(non_trainable_weights)\n",
      "\n",
      "  @property\n",
      "  def weights(self):\n",
      "    \"\"\"Returns the list of all layer variables/weights.\n",
      "\n",
      "    Returns:\n",
      "      A list of variables.\n",
      "    \"\"\"\n",
      "    return self.trainable_weights + self.non_trainable_weights\n",
      "\n",
      "  @property\n",
      "  @deprecation.deprecated(\n",
      "      date=None,\n",
      "      instructions='This property should not be used in TensorFlow 2.0, '\n",
      "      'as updates are applied automatically.')\n",
      "  @doc_controls.do_not_generate_docs\n",
      "  def updates(self):\n",
      "    if keras_tensor.keras_tensors_enabled():\n",
      "      return []\n",
      "\n",
      "    collected_updates = []\n",
      "    all_layers = self._flatten_layers()\n",
      "    with backend.get_graph().as_default():\n",
      "      for layer in all_layers:\n",
      "        if not layer.trainable and not layer.stateful:\n",
      "          continue\n",
      "        for u in layer._updates:\n",
      "          if callable(u):\n",
      "            u = u()\n",
      "          collected_updates.append(u)\n",
      "    return collected_updates\n",
      "\n",
      "  @property\n",
      "  def losses(self):\n",
      "    \"\"\"List of losses added using the `add_loss()` API.\n",
      "\n",
      "    Variable regularization tensors are created when this property is accessed,\n",
      "    so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      "    propagate gradients back to the corresponding variables.\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    >>> class MyLayer(tf.keras.layers.Layer):\n",
      "    ...   def call(self, inputs):\n",
      "    ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      "    ...     return inputs\n",
      "    >>> l = MyLayer()\n",
      "    >>> l(np.ones((10, 1)))\n",
      "    >>> l.losses\n",
      "    [1.0]\n",
      "\n",
      "    >>> inputs = tf.keras.Input(shape=(10,))\n",
      "    >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      "    >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      "    >>> model = tf.keras.Model(inputs, outputs)\n",
      "    >>> # Activity regularization.\n",
      "    >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      "    >>> model.losses\n",
      "    [<tf.Tensor 'Abs:0' shape=() dtype=float32>]\n",
      "\n",
      "    >>> inputs = tf.keras.Input(shape=(10,))\n",
      "    >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      "    >>> x = d(inputs)\n",
      "    >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      "    >>> model = tf.keras.Model(inputs, outputs)\n",
      "    >>> # Weight regularization.\n",
      "    >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      "    >>> model.losses\n",
      "    [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      "\n",
      "    Returns:\n",
      "      A list of tensors.\n",
      "    \"\"\"\n",
      "    collected_losses = []\n",
      "    for layer in self._flatten_layers():\n",
      "      # If any eager losses are present, we assume the model to be part of an\n",
      "      # eager training loop (either a custom one or the one used when\n",
      "      # `run_eagerly=True`) and so we always return just the eager losses.\n",
      "      if layer._eager_losses:\n",
      "        # Filter placeholder losses that may have been added by revived layers.\n",
      "        # (see base_layer_utils for details).\n",
      "        if (layer._eager_losses[0] is\n",
      "            not base_layer_utils.REVIVED_LOSS_PLACEHOLDER):\n",
      "          collected_losses.extend(layer._eager_losses)\n",
      "      else:\n",
      "        collected_losses.extend(layer._losses)\n",
      "      for regularizer in layer._callable_losses:\n",
      "        loss_tensor = regularizer()\n",
      "        if loss_tensor is not None:\n",
      "          collected_losses.append(loss_tensor)\n",
      "    return collected_losses\n",
      "\n",
      "  def add_loss(self, losses, **kwargs):\n",
      "    \"\"\"Add loss tensor(s), potentially dependent on layer inputs.\n",
      "\n",
      "    Some losses (for instance, activity regularization losses) may be dependent\n",
      "    on the inputs passed when calling a layer. Hence, when reusing the same\n",
      "    layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      "    be dependent on `a` and some on `b`. This method automatically keeps track\n",
      "    of dependencies.\n",
      "\n",
      "    This method can be used inside a subclassed layer or model's `call`\n",
      "    function, in which case `losses` should be a Tensor or list of Tensors.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    ```python\n",
      "    class MyLayer(tf.keras.layers.Layer):\n",
      "      def call(self, inputs):\n",
      "        self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      "        return inputs\n",
      "    ```\n",
      "\n",
      "    This method can also be called directly on a Functional Model during\n",
      "    construction. In this case, any loss Tensors passed to this Model must\n",
      "    be symbolic and be able to be traced back to the model's `Input`s. These\n",
      "    losses become part of the model's topology and are tracked in `get_config`.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    ```python\n",
      "    inputs = tf.keras.Input(shape=(10,))\n",
      "    x = tf.keras.layers.Dense(10)(inputs)\n",
      "    outputs = tf.keras.layers.Dense(1)(x)\n",
      "    model = tf.keras.Model(inputs, outputs)\n",
      "    # Activity regularization.\n",
      "    model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      "    ```\n",
      "\n",
      "    If this is not the case for your loss (if, for example, your loss references\n",
      "    a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      "    zero-argument lambda. These losses are not tracked as part of the model's\n",
      "    topology since they can't be serialized.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    ```python\n",
      "    inputs = tf.keras.Input(shape=(10,))\n",
      "    d = tf.keras.layers.Dense(10)\n",
      "    x = d(inputs)\n",
      "    outputs = tf.keras.layers.Dense(1)(x)\n",
      "    model = tf.keras.Model(inputs, outputs)\n",
      "    # Weight regularization.\n",
      "    model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      "    ```\n",
      "\n",
      "    Arguments:\n",
      "      losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      "        may also be zero-argument callables which create a loss tensor.\n",
      "      **kwargs: Additional keyword arguments for backward compatibility.\n",
      "        Accepted values:\n",
      "          inputs - Deprecated, will be automatically inferred.\n",
      "    \"\"\"\n",
      "    kwargs.pop('inputs', None)\n",
      "    if kwargs:\n",
      "      raise TypeError('Unknown keyword arguments: %s' % (kwargs.keys(),))\n",
      "\n",
      "    def _tag_callable(loss):\n",
      "      \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\"\n",
      "      if callable(loss):\n",
      "        # We run the loss without autocasting, as regularizers are often\n",
      "        # numerically unstable in float16.\n",
      "        with ops.enable_auto_cast_variables(None):\n",
      "          loss = loss()\n",
      "      if loss is None:\n",
      "        return None  # Will be filtered out when computing the .losses property\n",
      "      if not tensor_util.is_tensor(loss):\n",
      "        loss = ops.convert_to_tensor_v2(loss, dtype=backend.floatx())\n",
      "      loss._unconditional_loss = True  # pylint: disable=protected-access\n",
      "      return loss\n",
      "\n",
      "    losses = nest.flatten(losses)\n",
      "\n",
      "    callable_losses = []\n",
      "    eager_losses = []\n",
      "    symbolic_losses = []\n",
      "    for loss in losses:\n",
      "      if callable(loss):\n",
      "        callable_losses.append(functools.partial(_tag_callable, loss))\n",
      "        continue\n",
      "      if loss is None:\n",
      "        continue\n",
      "      if not tensor_util.is_tensor(loss) and not isinstance(\n",
      "          loss, keras_tensor.KerasTensor):\n",
      "        loss = ops.convert_to_tensor_v2(loss, dtype=backend.floatx())\n",
      "      # TF Functions should take the eager path.\n",
      "      if ((tf_utils.is_symbolic_tensor(loss) or\n",
      "           isinstance(loss, keras_tensor.KerasTensor)) and\n",
      "          not base_layer_utils.is_in_tf_function()):\n",
      "        symbolic_losses.append(loss)\n",
      "      elif tensor_util.is_tensor(loss):\n",
      "        eager_losses.append(loss)\n",
      "\n",
      "    self._callable_losses.extend(callable_losses)\n",
      "\n",
      "    in_call_context = base_layer_utils.call_context().in_call\n",
      "    if eager_losses and not in_call_context:\n",
      "      raise ValueError(\n",
      "          'Expected a symbolic Tensors or a callable for the loss value. '\n",
      "          'Please wrap your loss computation in a zero argument `lambda`.')\n",
      "\n",
      "    self._eager_losses.extend(eager_losses)\n",
      "\n",
      "    if in_call_context and not keras_tensor.keras_tensors_enabled():\n",
      "      for symbolic_loss in symbolic_losses:\n",
      "        self._losses.append(symbolic_loss)\n",
      "    else:\n",
      "      for symbolic_loss in symbolic_losses:\n",
      "        if getattr(self, '_is_graph_network', False):\n",
      "          self._graph_network_add_loss(symbolic_loss)\n",
      "        else:\n",
      "          # Possible a loss was added in a Layer's `build`.\n",
      "          self._losses.append(symbolic_loss)\n",
      "\n",
      "  def _clear_losses(self):\n",
      "    \"\"\"Used every step in eager to reset losses.\"\"\"\n",
      "    # Set to thread local directly to avoid Layer.__setattr__ overhead.\n",
      "    if not getattr(self, '_layers', None):  # Fast path for single Layer.\n",
      "      self._thread_local._eager_losses = []\n",
      "    else:\n",
      "      for layer in self._flatten_layers():\n",
      "        layer._thread_local._eager_losses = []\n",
      "\n",
      "  @property\n",
      "  def metrics(self):\n",
      "    \"\"\"List of metrics added using the `add_metric()` API.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    >>> input = tf.keras.layers.Input(shape=(3,))\n",
      "    >>> d = tf.keras.layers.Dense(2)\n",
      "    >>> output = d(input)\n",
      "    >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      "    >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      "    >>> [m.name for m in d.metrics]\n",
      "    ['max', 'min']\n",
      "\n",
      "    Returns:\n",
      "      A list of tensors.\n",
      "    \"\"\"\n",
      "    collected_metrics = []\n",
      "    for layer in self._flatten_layers():\n",
      "      with layer._metrics_lock:\n",
      "        collected_metrics.extend(layer._metrics)\n",
      "    return collected_metrics\n",
      "\n",
      "  def add_metric(self, value, name=None, **kwargs):\n",
      "    \"\"\"Adds metric tensor to the layer.\n",
      "\n",
      "    This method can be used inside the `call()` method of a subclassed layer\n",
      "    or model.\n",
      "\n",
      "    ```python\n",
      "    class MyMetricLayer(tf.keras.layers.Layer):\n",
      "      def __init__(self):\n",
      "        super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      "        self.mean = metrics_module.Mean(name='metric_1')\n",
      "\n",
      "      def call(self, inputs):\n",
      "        self.add_metric(self.mean(x))\n",
      "        self.add_metric(math_ops.reduce_sum(x), name='metric_2')\n",
      "        return inputs\n",
      "    ```\n",
      "\n",
      "    This method can also be called directly on a Functional Model during\n",
      "    construction. In this case, any tensor passed to this Model must\n",
      "    be symbolic and be able to be traced back to the model's `Input`s. These\n",
      "    metrics become part of the model's topology and are tracked when you\n",
      "    save the model via `save()`.\n",
      "\n",
      "    ```python\n",
      "    inputs = tf.keras.Input(shape=(10,))\n",
      "    x = tf.keras.layers.Dense(10)(inputs)\n",
      "    outputs = tf.keras.layers.Dense(1)(x)\n",
      "    model = tf.keras.Model(inputs, outputs)\n",
      "    model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      "    ```\n",
      "\n",
      "    Note: Calling `add_metric()` with the result of a metric object on a\n",
      "    Functional Model, as shown in the example below, is not supported. This is\n",
      "    because we cannot trace the metric result tensor back to the model's inputs.\n",
      "\n",
      "    ```python\n",
      "    inputs = tf.keras.Input(shape=(10,))\n",
      "    x = tf.keras.layers.Dense(10)(inputs)\n",
      "    outputs = tf.keras.layers.Dense(1)(x)\n",
      "    model = tf.keras.Model(inputs, outputs)\n",
      "    model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      "    ```\n",
      "\n",
      "    Args:\n",
      "      value: Metric tensor.\n",
      "      name: String metric name.\n",
      "      **kwargs: Additional keyword arguments for backward compatibility.\n",
      "        Accepted values:\n",
      "        `aggregation` - When the `value` tensor provided is not the result of\n",
      "        calling a `keras.Metric` instance, it will be aggregated by default\n",
      "        using a `keras.Metric.Mean`.\n",
      "    \"\"\"\n",
      "    kwargs_keys = list(kwargs.keys())\n",
      "    if (len(kwargs_keys) > 1 or\n",
      "        (len(kwargs_keys) == 1 and kwargs_keys[0] != 'aggregation')):\n",
      "      raise TypeError('Unknown keyword arguments: ', str(kwargs.keys()))\n",
      "\n",
      "    from_metric_obj = hasattr(value, '_metric_obj')\n",
      "    if keras_tensor.keras_tensors_enabled():\n",
      "      is_symbolic = isinstance(value, keras_tensor.KerasTensor)\n",
      "    else:\n",
      "      is_symbolic = tf_utils.is_symbolic_tensor(value)\n",
      "    in_call_context = base_layer_utils.call_context().in_call\n",
      "\n",
      "    if name is None and not from_metric_obj:\n",
      "      # Eg. `self.add_metric(math_ops.reduce_sum(x))`\n",
      "      # In eager mode, we use metric name to lookup a metric. Without a name,\n",
      "      # a new Mean metric wrapper will be created on every model/layer call.\n",
      "      # So, we raise an error when no name is provided.\n",
      "      # We will do the same for symbolic mode for consistency although a name\n",
      "      # will be generated if no name is provided.\n",
      "\n",
      "      # We will not raise this error in the foll use case for the sake of\n",
      "      # consistency as name in provided in the metric constructor.\n",
      "      # mean = metrics.Mean(name='my_metric')\n",
      "      # model.add_metric(mean(outputs))\n",
      "      raise ValueError('Please provide a name for your metric like '\n",
      "                       '`self.add_metric(tf.reduce_sum(inputs), '\n",
      "                       'name=\\'mean_activation\\')`')\n",
      "    elif from_metric_obj:\n",
      "      name = value._metric_obj.name\n",
      "\n",
      "    if not in_call_context and not is_symbolic:\n",
      "      raise ValueError('Expected a symbolic Tensor for the metric value, '\n",
      "                       'received: ' + str(value))\n",
      "\n",
      "    # If a metric was added in a Layer's `call` or `build`.\n",
      "    if in_call_context or not getattr(self, '_is_graph_network', False):\n",
      "      # TF Function path should take the eager path.\n",
      "\n",
      "      # If the given metric is available in `metrics` list we just update state\n",
      "      # on it, otherwise we create a new metric instance and\n",
      "      # add it to the `metrics` list.\n",
      "      metric_obj = getattr(value, '_metric_obj', None)\n",
      "      # Tensors that come from a Metric object already updated the Metric state.\n",
      "      should_update_state = not metric_obj\n",
      "      name = metric_obj.name if metric_obj else name\n",
      "\n",
      "      with self._metrics_lock:\n",
      "        match = self._get_existing_metric(name)\n",
      "        if match:\n",
      "          metric_obj = match\n",
      "        elif metric_obj:\n",
      "          self._metrics.append(metric_obj)\n",
      "        else:\n",
      "          from tensorflow.python.keras import metrics as metrics_mod  # pylint:disable=g-import-not-at-top\n",
      "          # Build the metric object with the value's dtype if it defines one\n",
      "          metric_obj = metrics_mod.Mean(\n",
      "              name=name, dtype=getattr(value, 'dtype', None))\n",
      "          self._metrics.append(metric_obj)\n",
      "\n",
      "      if should_update_state:\n",
      "        metric_obj(value)\n",
      "    else:\n",
      "      if from_metric_obj:\n",
      "        raise ValueError('Using the result of calling a `Metric` object '\n",
      "                         'when calling `add_metric` on a Functional '\n",
      "                         'Model is not supported. Please pass the '\n",
      "                         'Tensor to monitor directly.')\n",
      "\n",
      "      # Insert layers into the Keras Graph Network.\n",
      "      aggregation = None if from_metric_obj else 'mean'\n",
      "      self._graph_network_add_metric(value, aggregation, name)\n",
      "\n",
      "  @deprecation.deprecated_args(None, '`inputs` is now automatically inferred',\n",
      "                               'inputs')\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def add_update(self, updates, inputs=None):\n",
      "    \"\"\"Add update op(s), potentially dependent on layer inputs.\n",
      "\n",
      "    Weight updates (for instance, the updates of the moving mean and variance\n",
      "    in a BatchNormalization layer) may be dependent on the inputs passed\n",
      "    when calling a layer. Hence, when reusing the same layer on\n",
      "    different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      "    dependent on `a` and some on `b`. This method automatically keeps track\n",
      "    of dependencies.\n",
      "\n",
      "    This call is ignored when eager execution is enabled (in that case, variable\n",
      "    updates are run on the fly and thus do not need to be tracked for later\n",
      "    execution).\n",
      "\n",
      "    Arguments:\n",
      "      updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      "        that returns an update op. A zero-arg callable should be passed in\n",
      "        order to disable running the updates by setting `trainable=False`\n",
      "        on this Layer, when executing in Eager mode.\n",
      "      inputs: Deprecated, will be automatically inferred.\n",
      "    \"\"\"\n",
      "    call_context = base_layer_utils.call_context()\n",
      "    # No need to run updates during Functional API construction.\n",
      "    if call_context.in_keras_graph:\n",
      "      return\n",
      "\n",
      "    # Callable updates are disabled by setting `trainable=False`.\n",
      "    if not call_context.frozen:\n",
      "      for update in nest.flatten(updates):\n",
      "        if callable(update):\n",
      "          update()\n",
      "\n",
      "  def set_weights(self, weights):\n",
      "    \"\"\"Sets the weights of the layer, from Numpy arrays.\n",
      "\n",
      "    The weights of a layer represent the state of the layer. This function\n",
      "    sets the weight values from numpy arrays. The weight values should be\n",
      "    passed in the order they are created by the layer. Note that the layer's\n",
      "    weights must be instantiated before calling this function by calling\n",
      "    the layer.\n",
      "\n",
      "    For example, a Dense layer returns a list of two values-- per-output\n",
      "    weights and the bias value. These can be used to set the weights of another\n",
      "    Dense layer:\n",
      "\n",
      "    >>> a = tf.keras.layers.Dense(1,\n",
      "    ...   kernel_initializer=tf.constant_initializer(1.))\n",
      "    >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      "    >>> a.get_weights()\n",
      "    [array([[1.],\n",
      "           [1.],\n",
      "           [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      "    >>> b = tf.keras.layers.Dense(1,\n",
      "    ...   kernel_initializer=tf.constant_initializer(2.))\n",
      "    >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      "    >>> b.get_weights()\n",
      "    [array([[2.],\n",
      "           [2.],\n",
      "           [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      "    >>> b.set_weights(a.get_weights())\n",
      "    >>> b.get_weights()\n",
      "    [array([[1.],\n",
      "           [1.],\n",
      "           [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      "\n",
      "    Arguments:\n",
      "        weights: a list of Numpy arrays. The number\n",
      "            of arrays and their shape must match\n",
      "            number of the dimensions of the weights\n",
      "            of the layer (i.e. it should match the\n",
      "            output of `get_weights`).\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the provided weights list does not match the\n",
      "            layer's specifications.\n",
      "    \"\"\"\n",
      "    params = self.weights\n",
      "\n",
      "    expected_num_weights = 0\n",
      "    for param in params:\n",
      "      if isinstance(param, base_layer_utils.TrackableWeightHandler):\n",
      "        expected_num_weights += param.num_tensors\n",
      "      else:\n",
      "        expected_num_weights += 1\n",
      "\n",
      "    if expected_num_weights != len(weights):\n",
      "      raise ValueError(\n",
      "          'You called `set_weights(weights)` on layer \"%s\" '\n",
      "          'with a weight list of length %s, but the layer was '\n",
      "          'expecting %s weights. Provided weights: %s...' %\n",
      "          (self.name, len(weights), expected_num_weights, str(weights)[:50]))\n",
      "\n",
      "    weight_index = 0\n",
      "    weight_value_tuples = []\n",
      "    for param in params:\n",
      "      if isinstance(param, base_layer_utils.TrackableWeightHandler):\n",
      "        num_tensors = param.num_tensors\n",
      "        tensors = weights[weight_index:weight_index + num_tensors]\n",
      "        param.set_weights(tensors)\n",
      "        weight_index += num_tensors\n",
      "      else:\n",
      "        weight = weights[weight_index]\n",
      "        ref_shape = param.shape\n",
      "        if not ref_shape.is_compatible_with(weight.shape):\n",
      "          raise ValueError(\n",
      "              'Layer weight shape %s not compatible with provided weight '\n",
      "              'shape %s' % (ref_shape, weight.shape))\n",
      "        weight_value_tuples.append((param, weight))\n",
      "        weight_index += 1\n",
      "\n",
      "    backend.batch_set_value(weight_value_tuples)\n",
      "\n",
      "  def get_weights(self):\n",
      "    \"\"\"Returns the current weights of the layer.\n",
      "\n",
      "    The weights of a layer represent the state of the layer. This function\n",
      "    returns both trainable and non-trainable weight values associated with this\n",
      "    layer as a list of Numpy arrays, which can in turn be used to load state\n",
      "    into similarly parameterized layers.\n",
      "\n",
      "    For example, a Dense layer returns a list of two values-- per-output\n",
      "    weights and the bias value. These can be used to set the weights of another\n",
      "    Dense layer:\n",
      "\n",
      "    >>> a = tf.keras.layers.Dense(1,\n",
      "    ...   kernel_initializer=tf.constant_initializer(1.))\n",
      "    >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      "    >>> a.get_weights()\n",
      "    [array([[1.],\n",
      "           [1.],\n",
      "           [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      "    >>> b = tf.keras.layers.Dense(1,\n",
      "    ...   kernel_initializer=tf.constant_initializer(2.))\n",
      "    >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      "    >>> b.get_weights()\n",
      "    [array([[2.],\n",
      "           [2.],\n",
      "           [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      "    >>> b.set_weights(a.get_weights())\n",
      "    >>> b.get_weights()\n",
      "    [array([[1.],\n",
      "           [1.],\n",
      "           [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      "\n",
      "    Returns:\n",
      "        Weights values as a list of numpy arrays.\n",
      "    \"\"\"\n",
      "    weights = self.weights\n",
      "    output_weights = []\n",
      "    for weight in weights:\n",
      "      if isinstance(weight, base_layer_utils.TrackableWeightHandler):\n",
      "        output_weights.extend(weight.get_tensors())\n",
      "      else:\n",
      "        output_weights.append(weight)\n",
      "    return backend.batch_get_value(output_weights)\n",
      "\n",
      "  @deprecation.deprecated(\n",
      "      date=None, instructions='Please use `layer.updates` instead.')\n",
      "  @doc_controls.do_not_generate_docs\n",
      "  def get_updates_for(self, inputs):\n",
      "    \"\"\"Deprecated, do NOT use!\n",
      "\n",
      "    Retrieves updates relevant to a specific set of inputs.\n",
      "\n",
      "    Arguments:\n",
      "      inputs: Input tensor or list/tuple of input tensors.\n",
      "\n",
      "    Returns:\n",
      "      List of update ops of the layer that depend on `inputs`.\n",
      "    \"\"\"\n",
      "    return self.updates\n",
      "\n",
      "  @deprecation.deprecated(\n",
      "      date=None, instructions='Please use `layer.losses` instead.')\n",
      "  @doc_controls.do_not_generate_docs\n",
      "  def get_losses_for(self, inputs):\n",
      "    \"\"\"Deprecated, do NOT use!\n",
      "\n",
      "    Retrieves losses relevant to a specific set of inputs.\n",
      "\n",
      "    Arguments:\n",
      "      inputs: Input tensor or list/tuple of input tensors.\n",
      "\n",
      "    Returns:\n",
      "      List of loss tensors of the layer that depend on `inputs`.\n",
      "    \"\"\"\n",
      "    return self.losses\n",
      "\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def get_input_mask_at(self, node_index):\n",
      "    \"\"\"Retrieves the input mask tensor(s) of a layer at a given node.\n",
      "\n",
      "    Arguments:\n",
      "        node_index: Integer, index of the node\n",
      "            from which to retrieve the attribute.\n",
      "            E.g. `node_index=0` will correspond to the\n",
      "            first time the layer was called.\n",
      "\n",
      "    Returns:\n",
      "        A mask tensor\n",
      "        (or list of tensors if the layer has multiple inputs).\n",
      "    \"\"\"\n",
      "    inputs = self.get_input_at(node_index)\n",
      "    if isinstance(inputs, list):\n",
      "      return [getattr(x, '_keras_mask', None) for x in inputs]\n",
      "    else:\n",
      "      return getattr(inputs, '_keras_mask', None)\n",
      "\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def get_output_mask_at(self, node_index):\n",
      "    \"\"\"Retrieves the output mask tensor(s) of a layer at a given node.\n",
      "\n",
      "    Arguments:\n",
      "        node_index: Integer, index of the node\n",
      "            from which to retrieve the attribute.\n",
      "            E.g. `node_index=0` will correspond to the\n",
      "            first time the layer was called.\n",
      "\n",
      "    Returns:\n",
      "        A mask tensor\n",
      "        (or list of tensors if the layer has multiple outputs).\n",
      "    \"\"\"\n",
      "    output = self.get_output_at(node_index)\n",
      "    if isinstance(output, list):\n",
      "      return [getattr(x, '_keras_mask', None) for x in output]\n",
      "    else:\n",
      "      return getattr(output, '_keras_mask', None)\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def input_mask(self):\n",
      "    \"\"\"Retrieves the input mask tensor(s) of a layer.\n",
      "\n",
      "    Only applicable if the layer has exactly one inbound node,\n",
      "    i.e. if it is connected to one incoming layer.\n",
      "\n",
      "    Returns:\n",
      "        Input mask tensor (potentially None) or list of input\n",
      "        mask tensors.\n",
      "\n",
      "    Raises:\n",
      "        AttributeError: if the layer is connected to\n",
      "        more than one incoming layers.\n",
      "    \"\"\"\n",
      "    inputs = self.input\n",
      "    if isinstance(inputs, list):\n",
      "      return [getattr(x, '_keras_mask', None) for x in inputs]\n",
      "    else:\n",
      "      return getattr(inputs, '_keras_mask', None)\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def output_mask(self):\n",
      "    \"\"\"Retrieves the output mask tensor(s) of a layer.\n",
      "\n",
      "    Only applicable if the layer has exactly one inbound node,\n",
      "    i.e. if it is connected to one incoming layer.\n",
      "\n",
      "    Returns:\n",
      "        Output mask tensor (potentially None) or list of output\n",
      "        mask tensors.\n",
      "\n",
      "    Raises:\n",
      "        AttributeError: if the layer is connected to\n",
      "        more than one incoming layers.\n",
      "    \"\"\"\n",
      "    output = self.output\n",
      "    if isinstance(output, list):\n",
      "      return [getattr(x, '_keras_mask', None) for x in output]\n",
      "    else:\n",
      "      return getattr(output, '_keras_mask', None)\n",
      "\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def get_input_shape_at(self, node_index):\n",
      "    \"\"\"Retrieves the input shape(s) of a layer at a given node.\n",
      "\n",
      "    Arguments:\n",
      "        node_index: Integer, index of the node\n",
      "            from which to retrieve the attribute.\n",
      "            E.g. `node_index=0` will correspond to the\n",
      "            first time the layer was called.\n",
      "\n",
      "    Returns:\n",
      "        A shape tuple\n",
      "        (or list of shape tuples if the layer has multiple inputs).\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called in Eager mode.\n",
      "    \"\"\"\n",
      "    return self._get_node_attribute_at_index(node_index, 'input_shapes',\n",
      "                                             'input shape')\n",
      "\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def get_output_shape_at(self, node_index):\n",
      "    \"\"\"Retrieves the output shape(s) of a layer at a given node.\n",
      "\n",
      "    Arguments:\n",
      "        node_index: Integer, index of the node\n",
      "            from which to retrieve the attribute.\n",
      "            E.g. `node_index=0` will correspond to the\n",
      "            first time the layer was called.\n",
      "\n",
      "    Returns:\n",
      "        A shape tuple\n",
      "        (or list of shape tuples if the layer has multiple outputs).\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called in Eager mode.\n",
      "    \"\"\"\n",
      "    return self._get_node_attribute_at_index(node_index, 'output_shapes',\n",
      "                                             'output shape')\n",
      "\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def get_input_at(self, node_index):\n",
      "    \"\"\"Retrieves the input tensor(s) of a layer at a given node.\n",
      "\n",
      "    Arguments:\n",
      "        node_index: Integer, index of the node\n",
      "            from which to retrieve the attribute.\n",
      "            E.g. `node_index=0` will correspond to the\n",
      "            first time the layer was called.\n",
      "\n",
      "    Returns:\n",
      "        A tensor (or list of tensors if the layer has multiple inputs).\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called in Eager mode.\n",
      "    \"\"\"\n",
      "    return self._get_node_attribute_at_index(node_index, 'input_tensors',\n",
      "                                             'input')\n",
      "\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def get_output_at(self, node_index):\n",
      "    \"\"\"Retrieves the output tensor(s) of a layer at a given node.\n",
      "\n",
      "    Arguments:\n",
      "        node_index: Integer, index of the node\n",
      "            from which to retrieve the attribute.\n",
      "            E.g. `node_index=0` will correspond to the\n",
      "            first time the layer was called.\n",
      "\n",
      "    Returns:\n",
      "        A tensor (or list of tensors if the layer has multiple outputs).\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called in Eager mode.\n",
      "    \"\"\"\n",
      "    return self._get_node_attribute_at_index(node_index, 'output_tensors',\n",
      "                                             'output')\n",
      "\n",
      "  @property\n",
      "  def input(self):\n",
      "    \"\"\"Retrieves the input tensor(s) of a layer.\n",
      "\n",
      "    Only applicable if the layer has exactly one input,\n",
      "    i.e. if it is connected to one incoming layer.\n",
      "\n",
      "    Returns:\n",
      "        Input tensor or list of input tensors.\n",
      "\n",
      "    Raises:\n",
      "      RuntimeError: If called in Eager mode.\n",
      "      AttributeError: If no inbound nodes are found.\n",
      "    \"\"\"\n",
      "    if not self._inbound_nodes:\n",
      "      raise AttributeError('Layer ' + self.name +\n",
      "                           ' is not connected, no input to return.')\n",
      "    return self._get_node_attribute_at_index(0, 'input_tensors', 'input')\n",
      "\n",
      "  @property\n",
      "  def output(self):\n",
      "    \"\"\"Retrieves the output tensor(s) of a layer.\n",
      "\n",
      "    Only applicable if the layer has exactly one output,\n",
      "    i.e. if it is connected to one incoming layer.\n",
      "\n",
      "    Returns:\n",
      "      Output tensor or list of output tensors.\n",
      "\n",
      "    Raises:\n",
      "      AttributeError: if the layer is connected to more than one incoming\n",
      "        layers.\n",
      "      RuntimeError: if called in Eager mode.\n",
      "    \"\"\"\n",
      "    if not self._inbound_nodes:\n",
      "      raise AttributeError('Layer ' + self.name + ' has no inbound nodes.')\n",
      "    return self._get_node_attribute_at_index(0, 'output_tensors', 'output')\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def input_shape(self):\n",
      "    \"\"\"Retrieves the input shape(s) of a layer.\n",
      "\n",
      "    Only applicable if the layer has exactly one input,\n",
      "    i.e. if it is connected to one incoming layer, or if all inputs\n",
      "    have the same shape.\n",
      "\n",
      "    Returns:\n",
      "        Input shape, as an integer shape tuple\n",
      "        (or list of shape tuples, one tuple per input tensor).\n",
      "\n",
      "    Raises:\n",
      "        AttributeError: if the layer has no defined input_shape.\n",
      "        RuntimeError: if called in Eager mode.\n",
      "    \"\"\"\n",
      "    if not self._inbound_nodes:\n",
      "      raise AttributeError('The layer has never been called '\n",
      "                           'and thus has no defined input shape.')\n",
      "    all_input_shapes = set(\n",
      "        [str(node.input_shapes) for node in self._inbound_nodes])\n",
      "    if len(all_input_shapes) == 1:\n",
      "      return self._inbound_nodes[0].input_shapes\n",
      "    else:\n",
      "      raise AttributeError('The layer \"' + str(self.name) +\n",
      "                           ' has multiple inbound nodes, '\n",
      "                           'with different input shapes. Hence '\n",
      "                           'the notion of \"input shape\" is '\n",
      "                           'ill-defined for the layer. '\n",
      "                           'Use `get_input_shape_at(node_index)` '\n",
      "                           'instead.')\n",
      "\n",
      "  def count_params(self):\n",
      "    \"\"\"Count the total number of scalars composing the weights.\n",
      "\n",
      "    Returns:\n",
      "        An integer count.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: if the layer isn't yet built\n",
      "          (in which case its weights aren't yet defined).\n",
      "    \"\"\"\n",
      "    if not self.built:\n",
      "      if getattr(self, '_is_graph_network', False):\n",
      "        with tf_utils.maybe_init_scope(self):\n",
      "          self._maybe_build(self.inputs)\n",
      "      else:\n",
      "        raise ValueError('You tried to call `count_params` on ' + self.name +\n",
      "                         ', but the layer isn\\'t built. '\n",
      "                         'You can build it manually via: `' + self.name +\n",
      "                         '.build(batch_input_shape)`.')\n",
      "    return layer_utils.count_params(self.weights)\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def output_shape(self):\n",
      "    \"\"\"Retrieves the output shape(s) of a layer.\n",
      "\n",
      "    Only applicable if the layer has one output,\n",
      "    or if all outputs have the same shape.\n",
      "\n",
      "    Returns:\n",
      "        Output shape, as an integer shape tuple\n",
      "        (or list of shape tuples, one tuple per output tensor).\n",
      "\n",
      "    Raises:\n",
      "        AttributeError: if the layer has no defined output shape.\n",
      "        RuntimeError: if called in Eager mode.\n",
      "    \"\"\"\n",
      "    if not self._inbound_nodes:\n",
      "      raise AttributeError('The layer has never been called '\n",
      "                           'and thus has no defined output shape.')\n",
      "    all_output_shapes = set(\n",
      "        [str(node.output_shapes) for node in self._inbound_nodes])\n",
      "    if len(all_output_shapes) == 1:\n",
      "      return self._inbound_nodes[0].output_shapes\n",
      "    else:\n",
      "      raise AttributeError('The layer \"%s\"'\n",
      "                           ' has multiple inbound nodes, '\n",
      "                           'with different output shapes. Hence '\n",
      "                           'the notion of \"output shape\" is '\n",
      "                           'ill-defined for the layer. '\n",
      "                           'Use `get_output_shape_at(node_index)` '\n",
      "                           'instead.' % self.name)\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def inbound_nodes(self):\n",
      "    \"\"\"Deprecated, do NOT use! Only for compatibility with external Keras.\"\"\"\n",
      "    return self._inbound_nodes\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def outbound_nodes(self):\n",
      "    \"\"\"Deprecated, do NOT use! Only for compatibility with external Keras.\"\"\"\n",
      "    return self._outbound_nodes\n",
      "\n",
      "  ##############################################################################\n",
      "  # Methods & attributes below are public aliases of other methods.            #\n",
      "  ##############################################################################\n",
      "\n",
      "  @deprecation.deprecated(\n",
      "      date=None, instructions='Please use `layer.__call__` method instead.')\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def apply(self, inputs, *args, **kwargs):\n",
      "    \"\"\"Deprecated, do NOT use!\n",
      "\n",
      "    This is an alias of `self.__call__`.\n",
      "\n",
      "    Arguments:\n",
      "      inputs: Input tensor(s).\n",
      "      *args: additional positional arguments to be passed to `self.call`.\n",
      "      **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      "\n",
      "    Returns:\n",
      "      Output tensor(s).\n",
      "    \"\"\"\n",
      "    return self.__call__(inputs, *args, **kwargs)\n",
      "\n",
      "  @deprecation.deprecated(\n",
      "      date=None, instructions='Please use `layer.add_weight` method instead.')\n",
      "  @doc_controls.do_not_doc_inheritable\n",
      "  def add_variable(self, *args, **kwargs):\n",
      "    \"\"\"Deprecated, do NOT use! Alias for `add_weight`.\"\"\"\n",
      "    return self.add_weight(*args, **kwargs)\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_generate_docs\n",
      "  def variables(self):\n",
      "    \"\"\"Returns the list of all layer variables/weights.\n",
      "\n",
      "    Alias of `self.weights`.\n",
      "\n",
      "    Returns:\n",
      "      A list of variables.\n",
      "    \"\"\"\n",
      "    return self.weights\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_generate_docs\n",
      "  def trainable_variables(self):\n",
      "    return self.trainable_weights\n",
      "\n",
      "  @property\n",
      "  @doc_controls.do_not_generate_docs\n",
      "  def non_trainable_variables(self):\n",
      "    return self.non_trainable_weights\n",
      "\n",
      "  ##############################################################################\n",
      "  # Methods & attributes below are all private and only used by the framework. #\n",
      "  ##############################################################################\n",
      "\n",
      "  def _set_dtype_policy(self, dtype):\n",
      "    \"\"\"Sets self._dtype_policy.\"\"\"\n",
      "    if isinstance(dtype, policy.Policy):\n",
      "      self._dtype_policy = dtype\n",
      "    elif isinstance(dtype, dict):\n",
      "      self._dtype_policy = policy.deserialize(dtype)\n",
      "    elif dtype:\n",
      "      self._dtype_policy = policy.Policy(dtypes.as_dtype(dtype).name)\n",
      "    else:\n",
      "      self._dtype_policy = policy.global_policy()\n",
      "    if (self._dtype_policy.name == 'mixed_float16' and\n",
      "        not loss_scale_optimizer.strategy_supports_loss_scaling()):\n",
      "      # Although only loss scaling doesn't support certain strategies, to avoid\n",
      "      # confusion, we disallow the 'mixed_float16' policy with unsupported\n",
      "      # strategies. This is because 'mixed_float16' requires loss scaling for\n",
      "      # numeric stability.\n",
      "      strategy = ds_context.get_strategy()\n",
      "      raise ValueError('Mixed precision is not supported with the '\n",
      "                       'tf.distribute.Strategy: %s. Either stop using mixed '\n",
      "                       'precision by removing the use of the \"%s\" policy or '\n",
      "                       'use a different Strategy, e.g. a MirroredStrategy.' %\n",
      "                       (strategy.__class__.__name__, self._dtype_policy.name))\n",
      "\n",
      "    # This has no impact on the layer behavior, and is only used for printing\n",
      "    # warnings.\n",
      "    self._dtype_defaulted_to_floatx = (not dtype and\n",
      "                                       policy.policy_defaults_to_floatx())\n",
      "\n",
      "    # Performance optimization: cache the compute dtype as a Dtype object or\n",
      "    # None, so that str to Dtype conversion doesn't happen in Layer.__call__.\n",
      "    # TODO(b/157486353): Investigate returning DTypes in Policy.\n",
      "    if self._dtype_policy.compute_dtype:\n",
      "      self._compute_dtype_object = dtypes.as_dtype(\n",
      "          self._dtype_policy.compute_dtype)\n",
      "    else:\n",
      "      self._compute_dtype_object = None\n",
      "\n",
      "  # TODO(reedwm): Expose this property?\n",
      "  @property\n",
      "  def _compute_dtype(self):\n",
      "    \"\"\"The layer's compute dtype.\n",
      "\n",
      "    Unless mixed-precision is used, this is the same as `Layer.dtype`.\n",
      "\n",
      "    If self._autocast is True, layer's will cast floating-point inputs to this.\n",
      "\n",
      "    Returns:\n",
      "      The layer's compute dtype.\n",
      "    \"\"\"\n",
      "    return self._dtype_policy.compute_dtype\n",
      "\n",
      "  def _maybe_cast_inputs(self, inputs, input_list=None):\n",
      "    \"\"\"Maybe casts the inputs to the compute dtype.\n",
      "\n",
      "    If self._compute_dtype is floating-point, and self_autocast is True,\n",
      "    floating-point inputs are casted to self._compute_dtype.\n",
      "\n",
      "    Args:\n",
      "      inputs: Input tensor, or structure of input tensors.\n",
      "      input_list: Flat list of input tensors.\n",
      "\n",
      "    Returns:\n",
      "      `inputs`, but tensors may have been casted to self._compute_dtype\n",
      "    \"\"\"\n",
      "    if not input_list:\n",
      "      input_list = nest.flatten(inputs)\n",
      "\n",
      "    compute_dtype_object = self._compute_dtype_object\n",
      "    should_autocast = (\n",
      "        self._autocast and compute_dtype_object and\n",
      "        compute_dtype_object.is_floating)\n",
      "\n",
      "    if (should_autocast and\n",
      "        any(map(self._should_cast_single_input, input_list))):\n",
      "      # Only perform expensive `nest` operation when needed.\n",
      "      return nest.map_structure(self._cast_single_input, inputs)\n",
      "    else:\n",
      "      return inputs\n",
      "\n",
      "  def _should_cast_single_input(self, x):\n",
      "    if isinstance(x, _AUTOCAST_TYPES):\n",
      "      return (self._compute_dtype_object and\n",
      "              x.dtype != self._compute_dtype_object and x.dtype.is_floating)\n",
      "    return False\n",
      "\n",
      "  def _cast_single_input(self, x):\n",
      "    \"\"\"Cast a single Tensor or TensorSpec to the compute dtype.\"\"\"\n",
      "    if self._should_cast_single_input(x):\n",
      "      if self._dtype_defaulted_to_floatx:\n",
      "        self._warn_about_input_casting(x.dtype.base_dtype)\n",
      "      return math_ops.cast(x, self._compute_dtype_object)\n",
      "    else:\n",
      "      return x\n",
      "\n",
      "  def _warn_about_input_casting(self, input_dtype):\n",
      "    # self._already_warned_about_input_casting is only retrieved or set in this\n",
      "    # function.\n",
      "    already_warned = getattr(self, '_already_warned_about_input_casting', False)\n",
      "    if not already_warned:\n",
      "      tf_logging.warn(\n",
      "          \"Layer {self.name} is casting an input tensor from dtype \"\n",
      "          \"{input_dtype} to the layer's dtype of {layer_dtype}, which is new \"\n",
      "          \"behavior in TensorFlow 2.  The layer has dtype {layer_dtype} \"\n",
      "          'because its dtype defaults to floatx.\\n\\n'\n",
      "          \"\"\n",
      "          \"If you intended to run this layer in {layer_dtype}, you can safely \"\n",
      "          \"ignore this warning. If in doubt, this warning is likely only an \"\n",
      "          \"issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\\n\\n\"\n",
      "          \"\"\n",
      "          \"To change all layers to have dtype {input_dtype} by default, call \"\n",
      "          \"`tf.keras.backend.set_floatx('{input_dtype}')`. To change just this \"\n",
      "          \"layer, pass dtype='{input_dtype}' to the layer constructor. If you \"\n",
      "          \"are the author of this layer, you can disable autocasting by \"\n",
      "          \"passing autocast=False to the base Layer constructor.\\n\".format(\n",
      "              self=self,\n",
      "              input_dtype=input_dtype.name,\n",
      "              layer_dtype=self._compute_dtype))\n",
      "      self._already_warned_about_input_casting = True\n",
      "\n",
      "  # _dtype used to be an attribute set in the constructor. We still expose it\n",
      "  # because some clients still use it.\n",
      "  # TODO(reedwm): Deprecate, then remove the _dtype property.\n",
      "  @property\n",
      "  def _dtype(self):\n",
      "    # This is equivalent to returning self.dtype . We do not return self.dtype\n",
      "    # as it would cause infinite recursion in a few subclasses, which override\n",
      "    # \"dtype\" to return self._dtype.\n",
      "    return self._dtype_policy.variable_dtype\n",
      "\n",
      "  @_dtype.setter\n",
      "  def _dtype(self, value):\n",
      "    value = dtypes.as_dtype(value).name\n",
      "    self._set_dtype_policy(policy.Policy(value))\n",
      "\n",
      "  def _name_scope(self):\n",
      "    if not tf2.enabled():\n",
      "      return self.name\n",
      "    name_scope = self.name\n",
      "    current_name_scope = ops.get_name_scope()\n",
      "    if current_name_scope:\n",
      "      name_scope = current_name_scope + '/' + name_scope\n",
      "    if name_scope:\n",
      "      # Note that the trailing `/` prevents autogenerated\n",
      "      # numerical suffixes to get appended. It will also fully reset\n",
      "      # nested name scope (i.e. the outer name scope has no effect).\n",
      "      name_scope += '/'\n",
      "    return name_scope\n",
      "\n",
      "  def _init_set_name(self, name, zero_based=True):\n",
      "    if not name:\n",
      "      self._name = backend.unique_object_name(\n",
      "          generic_utils.to_snake_case(self.__class__.__name__),\n",
      "          zero_based=zero_based)\n",
      "    else:\n",
      "      self._name = name\n",
      "\n",
      "  def _get_existing_metric(self, name=None):\n",
      "    match = [m for m in self._metrics if m.name == name]\n",
      "    if not match:\n",
      "      return\n",
      "    if len(match) > 1:\n",
      "      raise ValueError(\n",
      "          'Please provide different names for the metrics you have added. '\n",
      "          'We found {} metrics with the name: \"{}\"'.format(len(match), name))\n",
      "    return match[0]\n",
      "\n",
      "  def _handle_weight_regularization(self, name, variable, regularizer):\n",
      "    \"\"\"Create lambdas which compute regularization losses.\"\"\"\n",
      "\n",
      "    def _loss_for_variable(v):\n",
      "      \"\"\"Creates a regularization loss `Tensor` for variable `v`.\"\"\"\n",
      "      with backend.name_scope(name + '/Regularizer'):\n",
      "        regularization = regularizer(v)\n",
      "      return regularization\n",
      "\n",
      "    if isinstance(variable, tf_variables.PartitionedVariable):\n",
      "      for v in variable:\n",
      "        self.add_loss(functools.partial(_loss_for_variable, v))\n",
      "    else:\n",
      "      self.add_loss(functools.partial(_loss_for_variable, variable))\n",
      "\n",
      "  def _handle_activity_regularization(self, inputs, outputs):\n",
      "    # Apply activity regularization.\n",
      "    # Note that it should be applied every time the layer creates a new\n",
      "    # output, since it is output-specific.\n",
      "    if self._activity_regularizer:\n",
      "      output_list = nest.flatten(outputs)\n",
      "      with backend.name_scope('ActivityRegularizer'):\n",
      "        for output in output_list:\n",
      "          activity_loss = self._activity_regularizer(output)\n",
      "          batch_size = math_ops.cast(\n",
      "              array_ops.shape(output)[0], activity_loss.dtype)\n",
      "          # Make activity regularization strength batch-agnostic.\n",
      "          mean_activity_loss = activity_loss / batch_size\n",
      "          self.add_loss(mean_activity_loss)\n",
      "\n",
      "  def _set_mask_metadata(self, inputs, outputs, previous_mask, build_graph):\n",
      "    # Many `Layer`s don't need to call `compute_mask`.\n",
      "    # This method is optimized to do as little work as needed for the common\n",
      "    # case.\n",
      "    if not self._supports_masking:\n",
      "      return\n",
      "\n",
      "    flat_outputs = nest.flatten(outputs)\n",
      "\n",
      "    mask_already_computed = (\n",
      "        getattr(self, '_compute_output_and_mask_jointly', False) or\n",
      "        all(getattr(x, '_keras_mask', None) is not None for x in flat_outputs))\n",
      "    if mask_already_computed:\n",
      "      if build_graph:\n",
      "        self._set_mask_keras_history_checked(flat_outputs)\n",
      "      return\n",
      "\n",
      "    output_masks = self.compute_mask(inputs, previous_mask)\n",
      "    if output_masks is None:\n",
      "      return\n",
      "\n",
      "    flat_masks = nest.flatten(output_masks)\n",
      "    for tensor, mask in zip(flat_outputs, flat_masks):\n",
      "      try:\n",
      "        tensor._keras_mask = mask\n",
      "      except AttributeError:\n",
      "        # C Type such as np.ndarray.\n",
      "        pass\n",
      "\n",
      "    if build_graph:\n",
      "      self._set_mask_keras_history_checked(flat_outputs)\n",
      "\n",
      "  def _set_mask_keras_history_checked(self, flat_outputs):\n",
      "    for output in flat_outputs:\n",
      "      if getattr(output, '_keras_mask', None) is not None:\n",
      "        # Do not track masks for `TensorFlowOpLayer` construction.\n",
      "        output._keras_mask._keras_history_checked = True\n",
      "\n",
      "  def _get_input_masks(self, inputs, input_list, args, kwargs):\n",
      "    if not self._supports_masking and not self._expects_mask_arg:\n",
      "      # Input masks only need to be retrieved if they are needed for `call`\n",
      "      # or `compute_mask`.\n",
      "      input_masks = None\n",
      "      implicit_mask = False\n",
      "    elif self._call_arg_was_passed('mask', args, kwargs):\n",
      "      input_masks = self._get_call_arg_value('mask', args, kwargs)\n",
      "      implicit_mask = False\n",
      "    else:\n",
      "      input_masks = [getattr(t, '_keras_mask', None) for t in input_list]\n",
      "      if all(mask is None for mask in input_masks):\n",
      "        input_masks = None\n",
      "        implicit_mask = False\n",
      "      else:\n",
      "        # Only do expensive `nest` op when masking is actually being used.\n",
      "        input_masks = nest.pack_sequence_as(inputs, input_masks)\n",
      "        implicit_mask = True\n",
      "    return input_masks, implicit_mask\n",
      "\n",
      "  def _call_arg_was_passed(self, arg_name, args, kwargs, inputs_in_args=False):\n",
      "    # Performance optimization: do no work in most common case.\n",
      "    if not args and not kwargs:\n",
      "      return False\n",
      "\n",
      "    if arg_name in kwargs:\n",
      "      return True\n",
      "    call_fn_args = self._call_fn_args\n",
      "    if not inputs_in_args:\n",
      "      # Ignore `inputs` arg.\n",
      "      call_fn_args = call_fn_args[1:]\n",
      "    return arg_name in dict(zip(call_fn_args, args))\n",
      "\n",
      "  def _get_call_arg_value(self, arg_name, args, kwargs, inputs_in_args=False):\n",
      "    if arg_name in kwargs:\n",
      "      return kwargs[arg_name]\n",
      "    call_fn_args = self._call_fn_args\n",
      "    if not inputs_in_args:\n",
      "      # Ignore `inputs` arg.\n",
      "      call_fn_args = call_fn_args[1:]\n",
      "    args_dict = dict(zip(call_fn_args, args))\n",
      "    return args_dict[arg_name]\n",
      "\n",
      "  def _set_call_arg_value(\n",
      "      self, arg_name, new_value, args,\n",
      "      kwargs, inputs_in_args=False, pop_kwarg_if_none=False):\n",
      "    arg_pos = self._call_fn_arg_positions.get(arg_name, None)\n",
      "    if arg_pos is not None:\n",
      "      if not inputs_in_args:\n",
      "        # Ignore `inputs` arg.\n",
      "        arg_pos = arg_pos - 1\n",
      "      if len(args) > arg_pos:\n",
      "        args = list(args)\n",
      "        args[arg_pos] = new_value\n",
      "        return tuple(args), kwargs\n",
      "    if new_value is None and pop_kwarg_if_none:\n",
      "      kwargs.pop(arg_name, None)\n",
      "    else:\n",
      "      kwargs[arg_name] = new_value\n",
      "    return args, kwargs\n",
      "\n",
      "  def _set_connectivity_metadata(self, args, kwargs, outputs):\n",
      "    # If the layer returns tensors from its inputs unmodified,\n",
      "    # we copy them to avoid loss of KerasHistory metadata.\n",
      "    flat_outputs = nest.flatten(outputs)\n",
      "    flat_inputs = nest.flatten((args, kwargs))\n",
      "    inputs_set = object_identity.ObjectIdentitySet(flat_inputs)\n",
      "    outputs_copy = []\n",
      "    for x in flat_outputs:\n",
      "      if x in inputs_set:\n",
      "        with backend.name_scope(self.name):\n",
      "          x = array_ops.identity(x)\n",
      "      outputs_copy.append(x)\n",
      "    outputs = nest.pack_sequence_as(outputs, outputs_copy)\n",
      "\n",
      "    # Create node, Node wires itself to inbound and outbound layers.\n",
      "    # The Node constructor actually updates this layer's self._inbound_nodes,\n",
      "    # sets _keras_history on the outputs, and adds itself to the\n",
      "    # `_outbound_nodes` of the layers that produced the inputs to this\n",
      "    # layer call.\n",
      "    node_module.Node(self, call_args=args, call_kwargs=kwargs, outputs=outputs)\n",
      "    return outputs\n",
      "\n",
      "  def _get_node_attribute_at_index(self, node_index, attr, attr_name):\n",
      "    \"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\n",
      "\n",
      "    This is used to implement the methods:\n",
      "        - get_input_shape_at\n",
      "        - get_output_shape_at\n",
      "        - get_input_at\n",
      "        etc...\n",
      "\n",
      "    Arguments:\n",
      "        node_index: Integer index of the node from which\n",
      "            to retrieve the attribute.\n",
      "        attr: Exact node attribute name.\n",
      "        attr_name: Human-readable attribute name, for error messages.\n",
      "\n",
      "    Returns:\n",
      "        The layer's attribute `attr` at the node of index `node_index`.\n",
      "\n",
      "    Raises:\n",
      "        RuntimeError: If the layer has no inbound nodes, or if called in Eager\n",
      "        mode.\n",
      "        ValueError: If the index provided does not match any node.\n",
      "    \"\"\"\n",
      "    if not self._inbound_nodes:\n",
      "      raise RuntimeError('The layer has never been called '\n",
      "                         'and thus has no defined ' + attr_name + '.')\n",
      "    if not len(self._inbound_nodes) > node_index:\n",
      "      raise ValueError('Asked to get ' + attr_name + ' at node ' +\n",
      "                       str(node_index) + ', but the layer has only ' +\n",
      "                       str(len(self._inbound_nodes)) + ' inbound nodes.')\n",
      "    values = getattr(self._inbound_nodes[node_index], attr)\n",
      "    if isinstance(values, list) and len(values) == 1:\n",
      "      return values[0]\n",
      "    else:\n",
      "      return values\n",
      "\n",
      "  def _maybe_build(self, inputs):\n",
      "    # Check input assumptions set before layer building, e.g. input rank.\n",
      "    if not self.built:\n",
      "      input_spec.assert_input_compatibility(\n",
      "          self.input_spec, inputs, self.name)\n",
      "      input_list = nest.flatten(inputs)\n",
      "      if input_list and self._dtype_policy.compute_dtype is None:\n",
      "        try:\n",
      "          dtype = input_list[0].dtype.base_dtype.name\n",
      "        except AttributeError:\n",
      "          pass\n",
      "        else:\n",
      "          self._set_dtype_policy(policy.Policy(dtype))\n",
      "      input_shapes = None\n",
      "      # Converts Tensors / CompositeTensors to TensorShapes.\n",
      "      if all(hasattr(x, 'shape') for x in input_list):\n",
      "        input_shapes = tf_utils.get_shapes(inputs)\n",
      "      else:\n",
      "        # Converts input shape to TensorShapes.\n",
      "        try:\n",
      "          input_shapes = tf_utils.convert_shapes(inputs, to_tuples=False)\n",
      "        except ValueError:\n",
      "          pass\n",
      "      # Only call `build` if the user has manually overridden the build method.\n",
      "      if not hasattr(self.build, '_is_default'):\n",
      "        # Any setup work performed only once should happen in an `init_scope`\n",
      "        # to avoid creating symbolic Tensors that will later pollute any eager\n",
      "        # operations.\n",
      "        with tf_utils.maybe_init_scope(self):\n",
      "          self.build(input_shapes)  # pylint:disable=not-callable\n",
      "      # We must set also ensure that the layer is marked as built, and the build\n",
      "      # shape is stored since user defined build functions may not be calling\n",
      "      # `super.build()`\n",
      "      Layer.build(self, input_shapes)\n",
      "\n",
      "    # Optionally load weight values specified at layer instantiation.\n",
      "    if self._initial_weights is not None:\n",
      "      if ops.executing_eagerly_outside_functions():\n",
      "        with ops.init_scope():\n",
      "          # Using `init_scope` since we want variable assignment in\n",
      "          # `set_weights` to be treated like variable initialization.\n",
      "          self.set_weights(self._initial_weights)\n",
      "      else:\n",
      "        self.set_weights(self._initial_weights)\n",
      "      self._initial_weights = None\n",
      "\n",
      "  def _symbolic_call(self, inputs):\n",
      "    input_shapes = nest.map_structure(lambda x: x.shape, inputs)\n",
      "    output_shapes = self.compute_output_shape(input_shapes)\n",
      "    # Convert to TensorShape so that nest.map_structure will not map into\n",
      "    # individual dim of the shape.\n",
      "    output_shapes = tf_utils.convert_shapes(output_shapes, to_tuples=False)\n",
      "\n",
      "    def _make_placeholder_like(shape):\n",
      "      ph = backend.placeholder(shape=shape, dtype=self.dtype)\n",
      "      ph._keras_mask = None\n",
      "      return ph\n",
      "    return nest.map_structure(_make_placeholder_like, output_shapes)\n",
      "\n",
      "  def _get_trainable_state(self):\n",
      "    \"\"\"Get the `trainable` state of each sublayer.\n",
      "\n",
      "    Returns:\n",
      "      A dict mapping all sublayers to their `trainable` value.\n",
      "    \"\"\"\n",
      "    trainable_state = weakref.WeakKeyDictionary()\n",
      "    for layer in self._flatten_layers():\n",
      "      trainable_state[layer] = layer.trainable\n",
      "    return trainable_state\n",
      "\n",
      "  def _set_trainable_state(self, trainable_state):\n",
      "    \"\"\"Set `trainable` state for each sublayer.\"\"\"\n",
      "    for layer in self._flatten_layers():\n",
      "      if layer in trainable_state:\n",
      "        layer.trainable = trainable_state[layer]\n",
      "\n",
      "  @property\n",
      "  def _obj_reference_counts(self):\n",
      "    \"\"\"A dictionary counting the number of attributes referencing an object.\"\"\"\n",
      "    self._maybe_create_attribute('_obj_reference_counts_dict',\n",
      "                                 object_identity.ObjectIdentityDictionary())\n",
      "    return self._obj_reference_counts_dict\n",
      "\n",
      "  @trackable.no_automatic_dependency_tracking\n",
      "  def _maybe_create_attribute(self, name, default_value):\n",
      "    \"\"\"Create the attribute with the default value if it hasn't been created.\n",
      "\n",
      "    This is useful for fields that is used for tracking purpose,\n",
      "    _trainable_weights, or _layers. Note that user could create a layer subclass\n",
      "    and assign an internal field before invoking the Layer.__init__(), the\n",
      "    __setattr__() need to create the tracking fields and __init__() need to not\n",
      "    override them.\n",
      "\n",
      "    Args:\n",
      "      name: String, the name of the attribute.\n",
      "      default_value: Object, the default value of the attribute.\n",
      "    \"\"\"\n",
      "    if not hasattr(self, name):\n",
      "      super(Layer, self).__setattr__(name, default_value)\n",
      "\n",
      "  def __delattr__(self, name):\n",
      "    # For any super.__delattr__() call, we will directly use the implementation\n",
      "    # in Trackable and skip the behavior in AutoTrackable. The Layer was\n",
      "    # originally use Trackable as base class, the change of using Module as base\n",
      "    # class forced us to have AutoTrackable in the class hierarchy. Skipping\n",
      "    # the __delattr__ and __setattr__ in AutoTrackable will keep the status quo.\n",
      "    existing_value = getattr(self, name, None)\n",
      "\n",
      "    # If this value is replacing an existing object assigned to an attribute, we\n",
      "    # should clean it out to avoid leaking memory. First we check if there are\n",
      "    # other attributes referencing it.\n",
      "    reference_counts = self._obj_reference_counts\n",
      "    if existing_value not in reference_counts:\n",
      "      super(tracking.AutoTrackable, self).__delattr__(name)\n",
      "      return\n",
      "\n",
      "    reference_count = reference_counts[existing_value]\n",
      "    if reference_count > 1:\n",
      "      # There are other remaining references. We can't remove this object from\n",
      "      # _layers etc.\n",
      "      reference_counts[existing_value] = reference_count - 1\n",
      "      super(tracking.AutoTrackable, self).__delattr__(name)\n",
      "      return\n",
      "    else:\n",
      "      # This is the last remaining reference.\n",
      "      del reference_counts[existing_value]\n",
      "\n",
      "    super(tracking.AutoTrackable, self).__delattr__(name)\n",
      "\n",
      "    if (isinstance(existing_value, Layer)\n",
      "        or trackable_layer_utils.has_weights(existing_value)):\n",
      "      super(tracking.AutoTrackable, self).__setattr__(\n",
      "          '_layers',\n",
      "          [l for l in self._layers if l is not existing_value])\n",
      "    if isinstance(existing_value, tf_variables.Variable):\n",
      "      super(tracking.AutoTrackable, self).__setattr__(\n",
      "          '_trainable_weights',\n",
      "          [w for w in self._trainable_weights if w is not existing_value])\n",
      "      super(tracking.AutoTrackable, self).__setattr__(\n",
      "          '_non_trainable_weights',\n",
      "          [w for w in self._non_trainable_weights if w is not existing_value])\n",
      "\n",
      "  def __setattr__(self, name, value):\n",
      "    if (name == '_self_setattr_tracking' or\n",
      "        not getattr(self, '_self_setattr_tracking', True) or\n",
      "        # Exclude @property.setters from tracking\n",
      "        hasattr(self.__class__, name)):\n",
      "      try:\n",
      "        super(tracking.AutoTrackable, self).__setattr__(name, value)\n",
      "      except AttributeError:\n",
      "        raise AttributeError(\n",
      "            ('Can\\'t set the attribute \"{}\", likely because it conflicts with '\n",
      "             'an existing read-only @property of the object. Please choose a '\n",
      "             'different name.').format(name))\n",
      "      return\n",
      "\n",
      "    # Keep track of trackable objects, for the needs of `Network.save_weights`.\n",
      "    value = data_structures.sticky_attribute_assignment(\n",
      "        trackable=self, value=value, name=name)\n",
      "\n",
      "    reference_counts = self._obj_reference_counts\n",
      "    reference_counts[value] = reference_counts.get(value, 0) + 1\n",
      "\n",
      "    # Clean out the old attribute, which clears _layers and _trainable_weights\n",
      "    # if necessary.\n",
      "    try:\n",
      "      self.__delattr__(name)\n",
      "    except AttributeError:\n",
      "      pass\n",
      "\n",
      "    # Keep track of metric instance created in subclassed layer.\n",
      "    from tensorflow.python.keras import metrics as metrics_module  # pylint: disable=g-import-not-at-top\n",
      "    for val in nest.flatten(value):\n",
      "      if isinstance(val, metrics_module.Metric) and hasattr(self, '_metrics'):\n",
      "        self._metrics.append(val)\n",
      "\n",
      "    # TODO(scottzhu): Need to track Module object as well for weight tracking.\n",
      "    # Be careful about metric if it becomes a Module in future.\n",
      "    # Append value to self._layers if relevant\n",
      "    if (getattr(self, '_auto_track_sub_layers', True) and\n",
      "        (isinstance(value, Layer) or trackable_layer_utils.has_weights(value))):\n",
      "      self._maybe_create_attribute('_layers', [])\n",
      "      # We need to check object identity to avoid de-duplicating empty\n",
      "      # container types which compare equal.\n",
      "      if not any((layer is value for layer in self._layers)):\n",
      "        self._layers.append(value)\n",
      "        if hasattr(value, '_use_resource_variables'):\n",
      "          # Legacy layers (V1 tf.layers) must always use\n",
      "          # resource variables.\n",
      "          value._use_resource_variables = True\n",
      "\n",
      "    # Append value to list of trainable / non-trainable weights if relevant\n",
      "    # TODO(b/125122625): This won't pick up on any variables added to a\n",
      "    # list/dict after creation.\n",
      "    for val in nest.flatten(value):\n",
      "      # TODO(b/126450014): Remove `_UnreadVariable` check here when assign ops\n",
      "      # no longer return True for isinstance Variable checks.\n",
      "      if not isinstance(val, tf_variables.Variable):\n",
      "        continue\n",
      "      if isinstance(val, resource_variable_ops._UnreadVariable):  # pylint: disable=protected-access\n",
      "        continue\n",
      "\n",
      "      # Users may add extra weights/variables\n",
      "      # simply by assigning them to attributes (invalid for graph networks)\n",
      "      self._maybe_create_attribute('_trainable_weights', [])\n",
      "      self._maybe_create_attribute('_non_trainable_weights', [])\n",
      "      if val.trainable:\n",
      "        if any(val is w for w in self._trainable_weights):\n",
      "          continue\n",
      "        self._trainable_weights.append(val)\n",
      "      else:\n",
      "        if any(val is w for w in self._non_trainable_weights):\n",
      "          continue\n",
      "        self._non_trainable_weights.append(val)\n",
      "\n",
      "      backend.track_variable(val)\n",
      "\n",
      "    # Skip the auto trackable from tf.Module to keep status quo. See the comment\n",
      "    # at __delattr__.\n",
      "    super(tracking.AutoTrackable, self).__setattr__(name, value)\n",
      "\n",
      "  def _gather_children_attribute(self, attribute):\n",
      "    assert attribute in {\n",
      "        'weights', 'trainable_weights', 'non_trainable_weights'\n",
      "    }\n",
      "    if hasattr(self, '_layers'):\n",
      "      nested_layers = trackable_layer_utils.filter_empty_layer_containers(\n",
      "          self._layers)\n",
      "      return list(\n",
      "          itertools.chain.from_iterable(\n",
      "              getattr(layer, attribute) for layer in nested_layers))\n",
      "    return []\n",
      "\n",
      "  def _flatten_layers(self, recursive=True, include_self=True):\n",
      "    if include_self:\n",
      "      yield self\n",
      "\n",
      "    # Only instantiate set and deque if needed.\n",
      "    layers_or_containers = getattr(self, '_layers', None)\n",
      "    if layers_or_containers:\n",
      "      seen_object_ids = set()\n",
      "      deque = collections.deque(layers_or_containers)\n",
      "      while deque:\n",
      "        layer_or_container = deque.popleft()\n",
      "\n",
      "        layer_or_container_id = id(layer_or_container)\n",
      "        if layer_or_container_id in seen_object_ids:\n",
      "          continue\n",
      "        seen_object_ids.add(layer_or_container_id)\n",
      "\n",
      "        if isinstance(layer_or_container, Layer):\n",
      "          yield layer_or_container\n",
      "          # Introspect recursively through sublayers.\n",
      "          if recursive:\n",
      "            sublayers = getattr(layer_or_container, '_layers', None)\n",
      "            if sublayers:\n",
      "              deque.extendleft(reversed(sublayers))\n",
      "        elif isinstance(layer_or_container,\n",
      "                        data_structures.TrackableDataStructure):\n",
      "          # Data structures are introspected even with `recursive=False`.\n",
      "          tracked_values = layer_or_container._values\n",
      "          if tracked_values:\n",
      "            deque.extendleft(reversed(tracked_values))\n",
      "\n",
      "  # This is a hack so that the is_layer (within\n",
      "  # training/trackable/layer_utils.py) check doesn't get the weights attr.\n",
      "  # TODO(b/110718070): Remove when fixed.\n",
      "  def _is_layer(self):\n",
      "    return True\n",
      "\n",
      "  def _init_call_fn_args(self):\n",
      "    # Clear cached call function arguments.\n",
      "    self.__class__._call_full_argspec.fget.cache.pop(self, None)\n",
      "    self.__class__._call_fn_args.fget.cache.pop(self, None)\n",
      "    self.__class__._call_accepts_kwargs.fget.cache.pop(self, None)\n",
      "\n",
      "    call_fn_args = self._call_fn_args\n",
      "    self._expects_training_arg = ('training' in call_fn_args or\n",
      "                                  self._call_accepts_kwargs)\n",
      "    # The default training arg will be any (non-None) default specified in the\n",
      "    # method signature, or None if no value is specified.\n",
      "    self._default_training_arg = self._call_fn_arg_defaults.get(\n",
      "        'training')\n",
      "    self._expects_mask_arg = ('mask' in call_fn_args or\n",
      "                              self._call_accepts_kwargs)\n",
      "\n",
      "  @property\n",
      "  @tracking.cached_per_instance\n",
      "  def _call_full_argspec(self):\n",
      "    # Argspec inspection is expensive and the call spec is used often, so it\n",
      "    # makes sense to cache the result.\n",
      "    return tf_inspect.getfullargspec(self.call)\n",
      "\n",
      "  @property\n",
      "  @tracking.cached_per_instance\n",
      "  def _call_fn_args(self):\n",
      "    all_args = self._call_full_argspec.args\n",
      "    # Scrub `self` that appears if a decorator was applied.\n",
      "    if all_args and all_args[0] == 'self':\n",
      "      return all_args[1:]\n",
      "    return all_args\n",
      "\n",
      "  @property\n",
      "  @tracking.cached_per_instance\n",
      "  def _call_fn_arg_defaults(self):\n",
      "    call_fn_args = self._call_fn_args\n",
      "    call_fn_defaults = self._call_full_argspec.defaults or []\n",
      "    defaults = dict()\n",
      "\n",
      "    # The call arg defaults are an n-tuple of the last n elements of the args\n",
      "    # list. (n = # of elements that have a default argument)\n",
      "    for i in range(-1 * len(call_fn_defaults), 0):\n",
      "      defaults[call_fn_args[i]] = call_fn_defaults[i]\n",
      "    return defaults\n",
      "\n",
      "  @property\n",
      "  @tracking.cached_per_instance\n",
      "  def _call_fn_arg_positions(self):\n",
      "    call_fn_arg_positions = dict()\n",
      "    for pos, arg in enumerate(self._call_fn_args):\n",
      "      call_fn_arg_positions[arg] = pos\n",
      "    return call_fn_arg_positions\n",
      "\n",
      "  @property\n",
      "  @tracking.cached_per_instance\n",
      "  def _call_accepts_kwargs(self):\n",
      "    return self._call_full_argspec.varkw is not None\n",
      "\n",
      "  @property\n",
      "  def _eager_losses(self):\n",
      "    # A list of loss values containing activity regularizers and losses\n",
      "    # manually added through `add_loss` during eager execution. It is cleared\n",
      "    # after every batch.\n",
      "    # Because we plan on eventually allowing a same model instance to be trained\n",
      "    # in eager mode or graph mode alternatively, we need to keep track of\n",
      "    # eager losses and symbolic losses via separate attributes.\n",
      "    if not hasattr(self._thread_local, '_eager_losses'):\n",
      "      self._thread_local._eager_losses = []\n",
      "    return self._thread_local._eager_losses\n",
      "\n",
      "  @_eager_losses.setter\n",
      "  def _eager_losses(self, losses):\n",
      "    self._thread_local._eager_losses = losses\n",
      "\n",
      "  def _dedup_weights(self, weights):\n",
      "    \"\"\"Dedupe weights while maintaining order as much as possible.\"\"\"\n",
      "    output, seen_weights = [], object_identity.ObjectIdentitySet()\n",
      "    for w in weights:\n",
      "      if w not in seen_weights:\n",
      "        output.append(w)\n",
      "        # Track the Variable's identity to avoid __eq__ issues.\n",
      "        seen_weights.add(w)\n",
      "    return output\n",
      "\n",
      "  def _split_out_first_arg(self, args, kwargs):\n",
      "    # Grab the argument corresponding to the first argument in the\n",
      "    # layer's `call` method spec. This will either be the first positional\n",
      "    # argument, or it will be provided as a keyword argument.\n",
      "    if args:\n",
      "      inputs = args[0]\n",
      "      args = args[1:]\n",
      "    elif self._call_fn_args[0] in kwargs:\n",
      "      kwargs = copy.copy(kwargs)\n",
      "      inputs = kwargs.pop(self._call_fn_args[0])\n",
      "    else:\n",
      "      raise ValueError(\n",
      "          'The first argument to `Layer.call` must always be passed.')\n",
      "    return inputs, args, kwargs\n",
      "\n",
      "  # SavedModel properties. Please see keras/saving/saved_model for details.\n",
      "\n",
      "  @trackable.no_automatic_dependency_tracking\n",
      "  def _set_save_spec(self, inputs):\n",
      "    if self._saved_model_inputs_spec is not None:\n",
      "      return  # Already set.\n",
      "\n",
      "    self._saved_model_inputs_spec = nest.map_structure(tf_utils.get_tensor_spec,\n",
      "                                                       inputs)\n",
      "\n",
      "  def _get_save_spec(self, dynamic_batch=True):\n",
      "    if self._saved_model_inputs_spec is None:\n",
      "      return None\n",
      "\n",
      "    return nest.map_structure(\n",
      "        lambda t: tf_utils.get_tensor_spec(t, dynamic_batch=dynamic_batch),\n",
      "        self._saved_model_inputs_spec)\n",
      "\n",
      "  @property\n",
      "  def _trackable_saved_model_saver(self):\n",
      "    return layer_serialization.LayerSavedModelSaver(self)\n",
      "\n",
      "  @property\n",
      "  def _object_identifier(self):\n",
      "    return self._trackable_saved_model_saver.object_identifier\n",
      "\n",
      "  @property\n",
      "  def _tracking_metadata(self):\n",
      "    return self._trackable_saved_model_saver.tracking_metadata\n",
      "\n",
      "  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n",
      "    return (self._trackable_saved_model_saver\n",
      "            .list_extra_dependencies_for_serialization(serialization_cache))\n",
      "\n",
      "  def _list_functions_for_serialization(self, serialization_cache):\n",
      "    return (self._trackable_saved_model_saver\n",
      "            .list_functions_for_serialization(serialization_cache))\n",
      "\n",
      "  def __getstate__(self):\n",
      "    # Override to support `copy.deepcopy` and pickling.\n",
      "    # Thread-local objects cannot be copied in Python 3, so pop these.\n",
      "    # Thread-local objects are used to cache losses in MirroredStrategy, and\n",
      "    # so shouldn't be copied.\n",
      "    state = self.__dict__.copy()\n",
      "    state.pop('_thread_local', None)\n",
      "    state.pop('_metrics_lock', None)\n",
      "    return state\n",
      "\n",
      "  def __setstate__(self, state):\n",
      "    state['_thread_local'] = threading.local()\n",
      "    state['_metrics_lock'] = threading.Lock()\n",
      "    # Bypass Trackable logic as `__dict__` already contains this info.\n",
      "    object.__setattr__(self, '__dict__', state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(tf.keras.layers.Layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x:\n",
    "    def __call__(self):\n",
    "        print('callble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callble\n"
     ]
    }
   ],
   "source": [
    "xx=x()\n",
    "xx() #class 인스턴스 callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### closure\n",
    "- 함수 안에 함수를 정의\n",
    "- 첫 인자에 따라서 뒤 인자에 영향 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(m):\n",
    "    def y(n):\n",
    "        return m+n\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.x.<locals>.y(n)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x(3)(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 객체를 closure처럼 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self,m):\n",
    "        self.m=m\n",
    "    def __call__(self,n):\n",
    "        return self.m+n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=A(3)  #인스턴스해서 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A(3)(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 클래스/함수 변경하는 방법 f(g(x))\n",
    "1. g 자체를 바꿈 \n",
    "    - 재활용 불가\n",
    "2. f(h(g(x))): g함수를 바꾸는 추가함수 만듦\n",
    "\n",
    "### Decorator\n",
    "- 직접 변경하지 않고 Decorator 로 변경해서 사용\n",
    "\n",
    "### function decorator\n",
    "- 함수를 인자로 받는데, 그 인자로 받은 함수를 변경하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(fun):\n",
    "    ''' 함수를 인자로 받음'''\n",
    "    print(fun('sun')) #fist class, callble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sun\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x(print) #print('sun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x(len) #len('sun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'u', 'n']\n"
     ]
    }
   ],
   "source": [
    "x(list)#list('sun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return을 합수로 만들기 위해서 함수를 받음\n",
    "\n",
    "- `----` 기능 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(fun):\n",
    "    def y():\n",
    "        print('----')\n",
    "        fun()\n",
    "        print('----')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "x(print)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec(fun):\n",
    "    ''' 2 빼기'''\n",
    "    def inner(a):\n",
    "        print('---')\n",
    "        return fun(a-2)\n",
    "    return inner\n",
    "\n",
    "def xx(a):\n",
    "    ''' 1 더하기'''\n",
    "    return a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(xx)(1) # (1+1)-2 =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decorator 사용해서 변경 할 수 있도록 만듦\n",
    "- 재사용 할 수 있도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dec\n",
    "def xx(a):\n",
    "    return a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec(fun):\n",
    "    def inner(*arg,**kwargs): #인자테크닉\n",
    "        print('---')\n",
    "        return fun(*arg,**kwargs)+1\n",
    "    return inner\n",
    "@dec\n",
    "def yy(a,b):\n",
    "    return a+b+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class decorator\n",
    "- class  에 class decorator 사용\n",
    "- class 받아서 기능을 바꾸어 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class A:\n",
    "    a: int\n",
    "    b: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=A(1,'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - tf decorator `@tf.function`\n",
    "    - 인자가 들어가는 경우\n",
    "    - ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decorator를 붙이면 원래 이름이 나오지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec(fun):\n",
    "    def inner(*arg,**kwargs):\n",
    "        print('---')\n",
    "        return fun(*arg,**kwargs)+1\n",
    "    return inner\n",
    "\n",
    "@dec\n",
    "def xx(a):\n",
    "    ''' docstring'''\n",
    "    return a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.dec(fun)>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 파일에 정의한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.dec.<locals>.inner(*arg, **kwargs)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inner'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.__module__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functools\n",
    "함수형 패러다임 3총사 중  한개\n",
    "### wraps\n",
    "- waps( function) 의 function의 이름, 정보 체크\n",
    "- debugging 용이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps,singledispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec(fun):\n",
    "    @wraps(fun)\n",
    "    def inner(*arg,**kwargs):\n",
    "        print('---')\n",
    "        return fun(*arg,**kwargs)+1\n",
    "    return inner\n",
    "\n",
    "@dec\n",
    "def xx(a):\n",
    "    ''' docstring'''\n",
    "    return a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.dec(fun)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.xx(a)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xx'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.__name__ #함수 xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' docstring'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.__module__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  decorator 인자 받는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para(n):\n",
    "    \n",
    "    def dec(fun):\n",
    "        @wraps(fun)\n",
    "        def inner(*arg,**kwargs):\n",
    "            print('---')\n",
    "            return fun(*arg,**kwargs)+n\n",
    "        return inner\n",
    "    \n",
    "    return dec\n",
    "\n",
    "@para(2)\n",
    "def xx(a):\n",
    "    ''' docstring: 최종 결과물에 3 더함'''\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx(2) #함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para(n=2):\n",
    "    def dec(fun):\n",
    "        @wraps(fun)\n",
    "        def inner(*arg,**kwargs):\n",
    "            print('---')\n",
    "            return fun(*arg,**kwargs)+n\n",
    "        return inner\n",
    "    return dec\n",
    "\n",
    "@para()\n",
    "def xx(a):\n",
    "    ''' docstring: 최종 결과물에 3 더함'''\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decorator 만드는데 decorator 사용\n",
    "\n",
    "### singledispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "@singledispatch\n",
    "def f(x):\n",
    "    pass\n",
    "\n",
    "@f.register(int)\n",
    "def _(x):\n",
    "    print('int')\n",
    "    \n",
    "@f.register(str)\n",
    "def _(x):\n",
    "    print('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int\n"
     ]
    }
   ],
   "source": [
    "f(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str\n"
     ]
    }
   ],
   "source": [
    "f('글자')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decorator 2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(fun):\n",
    "    def inner(*arg,**kwargs):\n",
    "        print('b')\n",
    "        return fun(*arg,**kwargs)\n",
    "    return inner\n",
    "\n",
    "def a(fun):\n",
    "    def inner(*arg,**kwargs):\n",
    "        print('a')\n",
    "        return fun(*arg,**kwargs)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "@b\n",
    "@a\n",
    "def x():\n",
    "    print('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행 순서 x, a, b ->b,a,x\n",
    "    - b(a(x()))\n",
    "    - stack 영역에 들어가서 b,a,x\n",
    "> super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "a\n",
      "x\n"
     ]
    }
   ],
   "source": [
    "x()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### higher-order function\n",
    " - 함수를 인자로 받거나 함수를 리턴 \n",
    " \n",
    "#### Syntax sugar: 단축 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxx(a,b):\n",
    "    ''' 인자 더하기 '''\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([1,2])\n",
    "y=np.array([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx([1,2],[4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy=np.vectorize(xxx)\n",
    "yyy([1,2],[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y #벡터합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.vectorize\n",
    "- decorator 로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def xxx(a,b):\n",
    "    ''' 인자 더하기 '''\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxx([1,2],[4,5]) #벡터합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3T\n",
    "- Technology 기술\n",
    "- Technique 기법\n",
    "- Tool 도구\n",
    "    - 이해하기 위해 3가지 필요 \n",
    "    \n",
    "    \n",
    "    딥러닝에서 가장 많이 쓰는 텐서플로우/파이토치(Tool)을 잘 사용하기 위해 python technique을 배움\n",
    "    - functional paradigm :데이터를 동시 처리, 이론 실제간의 간극 줄임\n",
    "        - iterator, generator\n",
    "            - next()  lazy evaluation\n",
    "        - map, filter,reduce \n",
    "\n",
    "- 파이썬에서는 객체지향, 항수형 패러다임 혼용함\n",
    "\n",
    "\n",
    "#### Technique\n",
    "- closure\n",
    "- `__init__, __call__`\n",
    "- 문보다 식\n",
    "- immutable\n",
    "\n",
    "### 클래스\n",
    "파이썬 공식문서 자습서의 클래스 참고: https://docs.python.org/ko/3/tutorial/classes.html#a-first-look-at-classes\n",
    "- 함수/클래스 정의 - 재사용\n",
    "    - 함수 scope LEGB\n",
    "    - 클래스 local scope\n",
    "    \n",
    "- callable 인스턴스 만들기\n",
    "- . 접근(attribute 참조)\n",
    "- 인스턴스화해서 인스턴스 객체로 사용\n",
    "- 클래스/인스턴스 변수/메서드\n",
    "    - 인스턴스에 없으면 클래스에서 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def x(self):\n",
    "        print('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.A.x(self)>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.x #function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method A.x of <__main__.A object at 0x7ffae264e210>>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.x #메서드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수와 매서드의 개념이 모호함\n",
    "    - 메서드:첫번째 인자 `self` 생략 \n",
    "    - 함수: 첫번째 인자 생략 불가\n",
    "- 파이썬은 모든것이 객체 : 매서드도 객체\n",
    "- 다중 상속\n",
    "    - super():메서도 결정 순서 (mro)\n",
    "- mangling : _class__attribute 로 바뀜\n",
    "    - descriptor로 비공개 변수 만들기 가능\n",
    "- class는 외부에서 접근 가능해서 동적으로 추가, 변경,삭제 가능\n",
    "    - vars()\n",
    "- iterator, generator \n",
    "    - 함수형 +객체지향\n",
    "- 디자인 패턴: 시스템화에서 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
