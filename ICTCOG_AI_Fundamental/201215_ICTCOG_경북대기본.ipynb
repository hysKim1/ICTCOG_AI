{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ICTCOG] 4기 경북대 기본반\n",
    "\n",
    "## 기계학습 (7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford mustang gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>europe</td>\n",
       "      <td>vw pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge rampage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford ranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevy s-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0    18.0          8         307.0       130.0    3504          12.0   \n",
       "1    15.0          8         350.0       165.0    3693          11.5   \n",
       "2    18.0          8         318.0       150.0    3436          11.0   \n",
       "3    16.0          8         304.0       150.0    3433          12.0   \n",
       "4    17.0          8         302.0       140.0    3449          10.5   \n",
       "..    ...        ...           ...         ...     ...           ...   \n",
       "393  27.0          4         140.0        86.0    2790          15.6   \n",
       "394  44.0          4          97.0        52.0    2130          24.6   \n",
       "395  32.0          4         135.0        84.0    2295          11.6   \n",
       "396  28.0          4         120.0        79.0    2625          18.6   \n",
       "397  31.0          4         119.0        82.0    2720          19.4   \n",
       "\n",
       "     model_year  origin                       name  \n",
       "0            70     usa  chevrolet chevelle malibu  \n",
       "1            70     usa          buick skylark 320  \n",
       "2            70     usa         plymouth satellite  \n",
       "3            70     usa              amc rebel sst  \n",
       "4            70     usa                ford torino  \n",
       "..          ...     ...                        ...  \n",
       "393          82     usa            ford mustang gl  \n",
       "394          82  europe                  vw pickup  \n",
       "395          82     usa              dodge rampage  \n",
       "396          82     usa                ford ranger  \n",
       "397          82     usa                 chevy s-10  \n",
       "\n",
       "[398 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg =sns.load_dataset('mpg')\n",
    "mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding\n",
    "- 문자를 숫자로 바꾸는 기법\n",
    "\n",
    "\n",
    "1. label encoding\n",
    "    - 학습 데이터로는 부적절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    50\n",
       "virginica     50\n",
       "setosa        50\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    1\n",
       "146    1\n",
       "147    1\n",
       "148    1\n",
       "149    1\n",
       "Name: species, Length: 150, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.species.map({'setosa':0,'virginica':1,'versicolor':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. one-hot encoding\n",
    "    - target 1 나머지 0으로 만든 벡터\n",
    "    - 차원이 늘어남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     setosa  versicolor  virginica\n",
       "0         1           0          0\n",
       "1         1           0          0\n",
       "2         1           0          0\n",
       "3         1           0          0\n",
       "4         1           0          0\n",
       "..      ...         ...        ...\n",
       "145       0           0          1\n",
       "146       0           0          1\n",
       "147       0           0          1\n",
       "148       0           0          1\n",
       "149       0           0          1\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection \n",
    "1. filter\n",
    "2. wrapper\n",
    "3. embeded\n",
    "\n",
    "\n",
    "### filter\n",
    "- 통계값을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris= sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb=SelectKBest(chi2, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2],\n",
       "       [4.6, 1.5, 0.2],\n",
       "       [5. , 1.4, 0.2],\n",
       "       [5.4, 1.7, 0.4],\n",
       "       [4.6, 1.4, 0.3],\n",
       "       [5. , 1.5, 0.2],\n",
       "       [4.4, 1.4, 0.2],\n",
       "       [4.9, 1.5, 0.1],\n",
       "       [5.4, 1.5, 0.2],\n",
       "       [4.8, 1.6, 0.2],\n",
       "       [4.8, 1.4, 0.1],\n",
       "       [4.3, 1.1, 0.1],\n",
       "       [5.8, 1.2, 0.2],\n",
       "       [5.7, 1.5, 0.4],\n",
       "       [5.4, 1.3, 0.4],\n",
       "       [5.1, 1.4, 0.3],\n",
       "       [5.7, 1.7, 0.3],\n",
       "       [5.1, 1.5, 0.3],\n",
       "       [5.4, 1.7, 0.2],\n",
       "       [5.1, 1.5, 0.4],\n",
       "       [4.6, 1. , 0.2],\n",
       "       [5.1, 1.7, 0.5],\n",
       "       [4.8, 1.9, 0.2],\n",
       "       [5. , 1.6, 0.2],\n",
       "       [5. , 1.6, 0.4],\n",
       "       [5.2, 1.5, 0.2],\n",
       "       [5.2, 1.4, 0.2],\n",
       "       [4.7, 1.6, 0.2],\n",
       "       [4.8, 1.6, 0.2],\n",
       "       [5.4, 1.5, 0.4],\n",
       "       [5.2, 1.5, 0.1],\n",
       "       [5.5, 1.4, 0.2],\n",
       "       [4.9, 1.5, 0.2],\n",
       "       [5. , 1.2, 0.2],\n",
       "       [5.5, 1.3, 0.2],\n",
       "       [4.9, 1.4, 0.1],\n",
       "       [4.4, 1.3, 0.2],\n",
       "       [5.1, 1.5, 0.2],\n",
       "       [5. , 1.3, 0.3],\n",
       "       [4.5, 1.3, 0.3],\n",
       "       [4.4, 1.3, 0.2],\n",
       "       [5. , 1.6, 0.6],\n",
       "       [5.1, 1.9, 0.4],\n",
       "       [4.8, 1.4, 0.3],\n",
       "       [5.1, 1.6, 0.2],\n",
       "       [4.6, 1.4, 0.2],\n",
       "       [5.3, 1.5, 0.2],\n",
       "       [5. , 1.4, 0.2],\n",
       "       [7. , 4.7, 1.4],\n",
       "       [6.4, 4.5, 1.5],\n",
       "       [6.9, 4.9, 1.5],\n",
       "       [5.5, 4. , 1.3],\n",
       "       [6.5, 4.6, 1.5],\n",
       "       [5.7, 4.5, 1.3],\n",
       "       [6.3, 4.7, 1.6],\n",
       "       [4.9, 3.3, 1. ],\n",
       "       [6.6, 4.6, 1.3],\n",
       "       [5.2, 3.9, 1.4],\n",
       "       [5. , 3.5, 1. ],\n",
       "       [5.9, 4.2, 1.5],\n",
       "       [6. , 4. , 1. ],\n",
       "       [6.1, 4.7, 1.4],\n",
       "       [5.6, 3.6, 1.3],\n",
       "       [6.7, 4.4, 1.4],\n",
       "       [5.6, 4.5, 1.5],\n",
       "       [5.8, 4.1, 1. ],\n",
       "       [6.2, 4.5, 1.5],\n",
       "       [5.6, 3.9, 1.1],\n",
       "       [5.9, 4.8, 1.8],\n",
       "       [6.1, 4. , 1.3],\n",
       "       [6.3, 4.9, 1.5],\n",
       "       [6.1, 4.7, 1.2],\n",
       "       [6.4, 4.3, 1.3],\n",
       "       [6.6, 4.4, 1.4],\n",
       "       [6.8, 4.8, 1.4],\n",
       "       [6.7, 5. , 1.7],\n",
       "       [6. , 4.5, 1.5],\n",
       "       [5.7, 3.5, 1. ],\n",
       "       [5.5, 3.8, 1.1],\n",
       "       [5.5, 3.7, 1. ],\n",
       "       [5.8, 3.9, 1.2],\n",
       "       [6. , 5.1, 1.6],\n",
       "       [5.4, 4.5, 1.5],\n",
       "       [6. , 4.5, 1.6],\n",
       "       [6.7, 4.7, 1.5],\n",
       "       [6.3, 4.4, 1.3],\n",
       "       [5.6, 4.1, 1.3],\n",
       "       [5.5, 4. , 1.3],\n",
       "       [5.5, 4.4, 1.2],\n",
       "       [6.1, 4.6, 1.4],\n",
       "       [5.8, 4. , 1.2],\n",
       "       [5. , 3.3, 1. ],\n",
       "       [5.6, 4.2, 1.3],\n",
       "       [5.7, 4.2, 1.2],\n",
       "       [5.7, 4.2, 1.3],\n",
       "       [6.2, 4.3, 1.3],\n",
       "       [5.1, 3. , 1.1],\n",
       "       [5.7, 4.1, 1.3],\n",
       "       [6.3, 6. , 2.5],\n",
       "       [5.8, 5.1, 1.9],\n",
       "       [7.1, 5.9, 2.1],\n",
       "       [6.3, 5.6, 1.8],\n",
       "       [6.5, 5.8, 2.2],\n",
       "       [7.6, 6.6, 2.1],\n",
       "       [4.9, 4.5, 1.7],\n",
       "       [7.3, 6.3, 1.8],\n",
       "       [6.7, 5.8, 1.8],\n",
       "       [7.2, 6.1, 2.5],\n",
       "       [6.5, 5.1, 2. ],\n",
       "       [6.4, 5.3, 1.9],\n",
       "       [6.8, 5.5, 2.1],\n",
       "       [5.7, 5. , 2. ],\n",
       "       [5.8, 5.1, 2.4],\n",
       "       [6.4, 5.3, 2.3],\n",
       "       [6.5, 5.5, 1.8],\n",
       "       [7.7, 6.7, 2.2],\n",
       "       [7.7, 6.9, 2.3],\n",
       "       [6. , 5. , 1.5],\n",
       "       [6.9, 5.7, 2.3],\n",
       "       [5.6, 4.9, 2. ],\n",
       "       [7.7, 6.7, 2. ],\n",
       "       [6.3, 4.9, 1.8],\n",
       "       [6.7, 5.7, 2.1],\n",
       "       [7.2, 6. , 1.8],\n",
       "       [6.2, 4.8, 1.8],\n",
       "       [6.1, 4.9, 1.8],\n",
       "       [6.4, 5.6, 2.1],\n",
       "       [7.2, 5.8, 1.6],\n",
       "       [7.4, 6.1, 1.9],\n",
       "       [7.9, 6.4, 2. ],\n",
       "       [6.4, 5.6, 2.2],\n",
       "       [6.3, 5.1, 1.5],\n",
       "       [6.1, 5.6, 1.4],\n",
       "       [7.7, 6.1, 2.3],\n",
       "       [6.3, 5.6, 2.4],\n",
       "       [6.4, 5.5, 1.8],\n",
       "       [6. , 4.8, 1.8],\n",
       "       [6.9, 5.4, 2.1],\n",
       "       [6.7, 5.6, 2.4],\n",
       "       [6.9, 5.1, 2.3],\n",
       "       [5.8, 5.1, 1.9],\n",
       "       [6.8, 5.9, 2.3],\n",
       "       [6.7, 5.7, 2.5],\n",
       "       [6.7, 5.2, 2.3],\n",
       "       [6.3, 5. , 1.9],\n",
       "       [6.5, 5.2, 2. ],\n",
       "       [6.2, 5.4, 2.3],\n",
       "       [5.9, 5.1, 1.8]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.fit_transform(iris.iloc[:,:-1],iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n",
       "       'species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sepal_width 제외 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris2=iris[['sepal_length', 'petal_length', 'petal_width','species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier()\n",
    "knn2=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원이 많을 수록 overfitting 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.93333333, 1.        , 1.        , 0.86666667,\n",
       "       0.93333333, 0.93333333, 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(KNeighborsClassifier(),iris.iloc[:,:-1],iris.species, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.93333333, 1.        , 0.93333333, 0.86666667,\n",
       "       0.93333333, 0.93333333, 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(KNeighborsClassifier(),iris2.iloc[:,:-1],iris2.species, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper\n",
    "- 알고리즘을 감싸서 사용\n",
    "### RFE\n",
    "- feature 1개씩 지워나가면서 모든 경우의수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe=RFE(KNeighborsClassifier(),n_features_to_select=3) \n",
    "#fit_transfrom 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe=RFE(LogisticRegression(),n_features_to_select=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MG/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.5, 1.4, 0.2],\n",
       "       [3. , 1.4, 0.2],\n",
       "       [3.2, 1.3, 0.2],\n",
       "       [3.1, 1.5, 0.2],\n",
       "       [3.6, 1.4, 0.2],\n",
       "       [3.9, 1.7, 0.4],\n",
       "       [3.4, 1.4, 0.3],\n",
       "       [3.4, 1.5, 0.2],\n",
       "       [2.9, 1.4, 0.2],\n",
       "       [3.1, 1.5, 0.1],\n",
       "       [3.7, 1.5, 0.2],\n",
       "       [3.4, 1.6, 0.2],\n",
       "       [3. , 1.4, 0.1],\n",
       "       [3. , 1.1, 0.1],\n",
       "       [4. , 1.2, 0.2],\n",
       "       [4.4, 1.5, 0.4],\n",
       "       [3.9, 1.3, 0.4],\n",
       "       [3.5, 1.4, 0.3],\n",
       "       [3.8, 1.7, 0.3],\n",
       "       [3.8, 1.5, 0.3],\n",
       "       [3.4, 1.7, 0.2],\n",
       "       [3.7, 1.5, 0.4],\n",
       "       [3.6, 1. , 0.2],\n",
       "       [3.3, 1.7, 0.5],\n",
       "       [3.4, 1.9, 0.2],\n",
       "       [3. , 1.6, 0.2],\n",
       "       [3.4, 1.6, 0.4],\n",
       "       [3.5, 1.5, 0.2],\n",
       "       [3.4, 1.4, 0.2],\n",
       "       [3.2, 1.6, 0.2],\n",
       "       [3.1, 1.6, 0.2],\n",
       "       [3.4, 1.5, 0.4],\n",
       "       [4.1, 1.5, 0.1],\n",
       "       [4.2, 1.4, 0.2],\n",
       "       [3.1, 1.5, 0.2],\n",
       "       [3.2, 1.2, 0.2],\n",
       "       [3.5, 1.3, 0.2],\n",
       "       [3.6, 1.4, 0.1],\n",
       "       [3. , 1.3, 0.2],\n",
       "       [3.4, 1.5, 0.2],\n",
       "       [3.5, 1.3, 0.3],\n",
       "       [2.3, 1.3, 0.3],\n",
       "       [3.2, 1.3, 0.2],\n",
       "       [3.5, 1.6, 0.6],\n",
       "       [3.8, 1.9, 0.4],\n",
       "       [3. , 1.4, 0.3],\n",
       "       [3.8, 1.6, 0.2],\n",
       "       [3.2, 1.4, 0.2],\n",
       "       [3.7, 1.5, 0.2],\n",
       "       [3.3, 1.4, 0.2],\n",
       "       [3.2, 4.7, 1.4],\n",
       "       [3.2, 4.5, 1.5],\n",
       "       [3.1, 4.9, 1.5],\n",
       "       [2.3, 4. , 1.3],\n",
       "       [2.8, 4.6, 1.5],\n",
       "       [2.8, 4.5, 1.3],\n",
       "       [3.3, 4.7, 1.6],\n",
       "       [2.4, 3.3, 1. ],\n",
       "       [2.9, 4.6, 1.3],\n",
       "       [2.7, 3.9, 1.4],\n",
       "       [2. , 3.5, 1. ],\n",
       "       [3. , 4.2, 1.5],\n",
       "       [2.2, 4. , 1. ],\n",
       "       [2.9, 4.7, 1.4],\n",
       "       [2.9, 3.6, 1.3],\n",
       "       [3.1, 4.4, 1.4],\n",
       "       [3. , 4.5, 1.5],\n",
       "       [2.7, 4.1, 1. ],\n",
       "       [2.2, 4.5, 1.5],\n",
       "       [2.5, 3.9, 1.1],\n",
       "       [3.2, 4.8, 1.8],\n",
       "       [2.8, 4. , 1.3],\n",
       "       [2.5, 4.9, 1.5],\n",
       "       [2.8, 4.7, 1.2],\n",
       "       [2.9, 4.3, 1.3],\n",
       "       [3. , 4.4, 1.4],\n",
       "       [2.8, 4.8, 1.4],\n",
       "       [3. , 5. , 1.7],\n",
       "       [2.9, 4.5, 1.5],\n",
       "       [2.6, 3.5, 1. ],\n",
       "       [2.4, 3.8, 1.1],\n",
       "       [2.4, 3.7, 1. ],\n",
       "       [2.7, 3.9, 1.2],\n",
       "       [2.7, 5.1, 1.6],\n",
       "       [3. , 4.5, 1.5],\n",
       "       [3.4, 4.5, 1.6],\n",
       "       [3.1, 4.7, 1.5],\n",
       "       [2.3, 4.4, 1.3],\n",
       "       [3. , 4.1, 1.3],\n",
       "       [2.5, 4. , 1.3],\n",
       "       [2.6, 4.4, 1.2],\n",
       "       [3. , 4.6, 1.4],\n",
       "       [2.6, 4. , 1.2],\n",
       "       [2.3, 3.3, 1. ],\n",
       "       [2.7, 4.2, 1.3],\n",
       "       [3. , 4.2, 1.2],\n",
       "       [2.9, 4.2, 1.3],\n",
       "       [2.9, 4.3, 1.3],\n",
       "       [2.5, 3. , 1.1],\n",
       "       [2.8, 4.1, 1.3],\n",
       "       [3.3, 6. , 2.5],\n",
       "       [2.7, 5.1, 1.9],\n",
       "       [3. , 5.9, 2.1],\n",
       "       [2.9, 5.6, 1.8],\n",
       "       [3. , 5.8, 2.2],\n",
       "       [3. , 6.6, 2.1],\n",
       "       [2.5, 4.5, 1.7],\n",
       "       [2.9, 6.3, 1.8],\n",
       "       [2.5, 5.8, 1.8],\n",
       "       [3.6, 6.1, 2.5],\n",
       "       [3.2, 5.1, 2. ],\n",
       "       [2.7, 5.3, 1.9],\n",
       "       [3. , 5.5, 2.1],\n",
       "       [2.5, 5. , 2. ],\n",
       "       [2.8, 5.1, 2.4],\n",
       "       [3.2, 5.3, 2.3],\n",
       "       [3. , 5.5, 1.8],\n",
       "       [3.8, 6.7, 2.2],\n",
       "       [2.6, 6.9, 2.3],\n",
       "       [2.2, 5. , 1.5],\n",
       "       [3.2, 5.7, 2.3],\n",
       "       [2.8, 4.9, 2. ],\n",
       "       [2.8, 6.7, 2. ],\n",
       "       [2.7, 4.9, 1.8],\n",
       "       [3.3, 5.7, 2.1],\n",
       "       [3.2, 6. , 1.8],\n",
       "       [2.8, 4.8, 1.8],\n",
       "       [3. , 4.9, 1.8],\n",
       "       [2.8, 5.6, 2.1],\n",
       "       [3. , 5.8, 1.6],\n",
       "       [2.8, 6.1, 1.9],\n",
       "       [3.8, 6.4, 2. ],\n",
       "       [2.8, 5.6, 2.2],\n",
       "       [2.8, 5.1, 1.5],\n",
       "       [2.6, 5.6, 1.4],\n",
       "       [3. , 6.1, 2.3],\n",
       "       [3.4, 5.6, 2.4],\n",
       "       [3.1, 5.5, 1.8],\n",
       "       [3. , 4.8, 1.8],\n",
       "       [3.1, 5.4, 2.1],\n",
       "       [3.1, 5.6, 2.4],\n",
       "       [3.1, 5.1, 2.3],\n",
       "       [2.7, 5.1, 1.9],\n",
       "       [3.2, 5.9, 2.3],\n",
       "       [3.3, 5.7, 2.5],\n",
       "       [3. , 5.2, 2.3],\n",
       "       [2.5, 5. , 1.9],\n",
       "       [3. , 5.2, 2. ],\n",
       "       [3.4, 5.4, 2.3],\n",
       "       [3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe.fit_transform(iris.iloc[:,:-1], iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe=RFE(DecisionTreeClassifier(),n_features_to_select=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.5, 1.4, 0.2],\n",
       "       [3. , 1.4, 0.2],\n",
       "       [3.2, 1.3, 0.2],\n",
       "       [3.1, 1.5, 0.2],\n",
       "       [3.6, 1.4, 0.2],\n",
       "       [3.9, 1.7, 0.4],\n",
       "       [3.4, 1.4, 0.3],\n",
       "       [3.4, 1.5, 0.2],\n",
       "       [2.9, 1.4, 0.2],\n",
       "       [3.1, 1.5, 0.1],\n",
       "       [3.7, 1.5, 0.2],\n",
       "       [3.4, 1.6, 0.2],\n",
       "       [3. , 1.4, 0.1],\n",
       "       [3. , 1.1, 0.1],\n",
       "       [4. , 1.2, 0.2],\n",
       "       [4.4, 1.5, 0.4],\n",
       "       [3.9, 1.3, 0.4],\n",
       "       [3.5, 1.4, 0.3],\n",
       "       [3.8, 1.7, 0.3],\n",
       "       [3.8, 1.5, 0.3],\n",
       "       [3.4, 1.7, 0.2],\n",
       "       [3.7, 1.5, 0.4],\n",
       "       [3.6, 1. , 0.2],\n",
       "       [3.3, 1.7, 0.5],\n",
       "       [3.4, 1.9, 0.2],\n",
       "       [3. , 1.6, 0.2],\n",
       "       [3.4, 1.6, 0.4],\n",
       "       [3.5, 1.5, 0.2],\n",
       "       [3.4, 1.4, 0.2],\n",
       "       [3.2, 1.6, 0.2],\n",
       "       [3.1, 1.6, 0.2],\n",
       "       [3.4, 1.5, 0.4],\n",
       "       [4.1, 1.5, 0.1],\n",
       "       [4.2, 1.4, 0.2],\n",
       "       [3.1, 1.5, 0.2],\n",
       "       [3.2, 1.2, 0.2],\n",
       "       [3.5, 1.3, 0.2],\n",
       "       [3.6, 1.4, 0.1],\n",
       "       [3. , 1.3, 0.2],\n",
       "       [3.4, 1.5, 0.2],\n",
       "       [3.5, 1.3, 0.3],\n",
       "       [2.3, 1.3, 0.3],\n",
       "       [3.2, 1.3, 0.2],\n",
       "       [3.5, 1.6, 0.6],\n",
       "       [3.8, 1.9, 0.4],\n",
       "       [3. , 1.4, 0.3],\n",
       "       [3.8, 1.6, 0.2],\n",
       "       [3.2, 1.4, 0.2],\n",
       "       [3.7, 1.5, 0.2],\n",
       "       [3.3, 1.4, 0.2],\n",
       "       [3.2, 4.7, 1.4],\n",
       "       [3.2, 4.5, 1.5],\n",
       "       [3.1, 4.9, 1.5],\n",
       "       [2.3, 4. , 1.3],\n",
       "       [2.8, 4.6, 1.5],\n",
       "       [2.8, 4.5, 1.3],\n",
       "       [3.3, 4.7, 1.6],\n",
       "       [2.4, 3.3, 1. ],\n",
       "       [2.9, 4.6, 1.3],\n",
       "       [2.7, 3.9, 1.4],\n",
       "       [2. , 3.5, 1. ],\n",
       "       [3. , 4.2, 1.5],\n",
       "       [2.2, 4. , 1. ],\n",
       "       [2.9, 4.7, 1.4],\n",
       "       [2.9, 3.6, 1.3],\n",
       "       [3.1, 4.4, 1.4],\n",
       "       [3. , 4.5, 1.5],\n",
       "       [2.7, 4.1, 1. ],\n",
       "       [2.2, 4.5, 1.5],\n",
       "       [2.5, 3.9, 1.1],\n",
       "       [3.2, 4.8, 1.8],\n",
       "       [2.8, 4. , 1.3],\n",
       "       [2.5, 4.9, 1.5],\n",
       "       [2.8, 4.7, 1.2],\n",
       "       [2.9, 4.3, 1.3],\n",
       "       [3. , 4.4, 1.4],\n",
       "       [2.8, 4.8, 1.4],\n",
       "       [3. , 5. , 1.7],\n",
       "       [2.9, 4.5, 1.5],\n",
       "       [2.6, 3.5, 1. ],\n",
       "       [2.4, 3.8, 1.1],\n",
       "       [2.4, 3.7, 1. ],\n",
       "       [2.7, 3.9, 1.2],\n",
       "       [2.7, 5.1, 1.6],\n",
       "       [3. , 4.5, 1.5],\n",
       "       [3.4, 4.5, 1.6],\n",
       "       [3.1, 4.7, 1.5],\n",
       "       [2.3, 4.4, 1.3],\n",
       "       [3. , 4.1, 1.3],\n",
       "       [2.5, 4. , 1.3],\n",
       "       [2.6, 4.4, 1.2],\n",
       "       [3. , 4.6, 1.4],\n",
       "       [2.6, 4. , 1.2],\n",
       "       [2.3, 3.3, 1. ],\n",
       "       [2.7, 4.2, 1.3],\n",
       "       [3. , 4.2, 1.2],\n",
       "       [2.9, 4.2, 1.3],\n",
       "       [2.9, 4.3, 1.3],\n",
       "       [2.5, 3. , 1.1],\n",
       "       [2.8, 4.1, 1.3],\n",
       "       [3.3, 6. , 2.5],\n",
       "       [2.7, 5.1, 1.9],\n",
       "       [3. , 5.9, 2.1],\n",
       "       [2.9, 5.6, 1.8],\n",
       "       [3. , 5.8, 2.2],\n",
       "       [3. , 6.6, 2.1],\n",
       "       [2.5, 4.5, 1.7],\n",
       "       [2.9, 6.3, 1.8],\n",
       "       [2.5, 5.8, 1.8],\n",
       "       [3.6, 6.1, 2.5],\n",
       "       [3.2, 5.1, 2. ],\n",
       "       [2.7, 5.3, 1.9],\n",
       "       [3. , 5.5, 2.1],\n",
       "       [2.5, 5. , 2. ],\n",
       "       [2.8, 5.1, 2.4],\n",
       "       [3.2, 5.3, 2.3],\n",
       "       [3. , 5.5, 1.8],\n",
       "       [3.8, 6.7, 2.2],\n",
       "       [2.6, 6.9, 2.3],\n",
       "       [2.2, 5. , 1.5],\n",
       "       [3.2, 5.7, 2.3],\n",
       "       [2.8, 4.9, 2. ],\n",
       "       [2.8, 6.7, 2. ],\n",
       "       [2.7, 4.9, 1.8],\n",
       "       [3.3, 5.7, 2.1],\n",
       "       [3.2, 6. , 1.8],\n",
       "       [2.8, 4.8, 1.8],\n",
       "       [3. , 4.9, 1.8],\n",
       "       [2.8, 5.6, 2.1],\n",
       "       [3. , 5.8, 1.6],\n",
       "       [2.8, 6.1, 1.9],\n",
       "       [3.8, 6.4, 2. ],\n",
       "       [2.8, 5.6, 2.2],\n",
       "       [2.8, 5.1, 1.5],\n",
       "       [2.6, 5.6, 1.4],\n",
       "       [3. , 6.1, 2.3],\n",
       "       [3.4, 5.6, 2.4],\n",
       "       [3.1, 5.5, 1.8],\n",
       "       [3. , 4.8, 1.8],\n",
       "       [3.1, 5.4, 2.1],\n",
       "       [3.1, 5.6, 2.4],\n",
       "       [3.1, 5.1, 2.3],\n",
       "       [2.7, 5.1, 1.9],\n",
       "       [3.2, 5.9, 2.3],\n",
       "       [3.3, 5.7, 2.5],\n",
       "       [3. , 5.2, 2.3],\n",
       "       [2.5, 5. , 1.9],\n",
       "       [3. , 5.2, 2. ],\n",
       "       [3.4, 5.4, 2.3],\n",
       "       [3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe.fit_transform(iris.iloc[:,:-1], iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeded\n",
    "- 알고리즘에서 성능 유지하면서 덜 중요한 feature 제외\n",
    "### DecisionTree\n",
    "- 의사 결정 나무: 특정 조건에 따라 불확실성을 줄이는 가장 \n",
    "- entropy: 불확실성을 낮춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(iris.iloc[:,:-1], iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전에 anaconda에서 설치해야 그려짐"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 199.32, 'X[2] <= 2.45\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n",
       " Text(141.64615384615385, 163.07999999999998, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n",
       " Text(193.15384615384616, 163.07999999999998, 'X[3] <= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n",
       " Text(103.01538461538462, 126.83999999999999, 'X[2] <= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n",
       " Text(51.50769230769231, 90.6, 'X[3] <= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'),\n",
       " Text(25.753846153846155, 54.359999999999985, 'gini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'),\n",
       " Text(77.26153846153846, 54.359999999999985, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n",
       " Text(154.52307692307693, 90.6, 'X[3] <= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'),\n",
       " Text(128.76923076923077, 54.359999999999985, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'),\n",
       " Text(180.27692307692308, 54.359999999999985, 'X[0] <= 6.95\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'),\n",
       " Text(154.52307692307693, 18.119999999999976, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'),\n",
       " Text(206.03076923076924, 18.119999999999976, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n",
       " Text(283.2923076923077, 126.83999999999999, 'X[2] <= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'),\n",
       " Text(257.53846153846155, 90.6, 'X[0] <= 5.95\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'),\n",
       " Text(231.7846153846154, 54.359999999999985, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0]'),\n",
       " Text(283.2923076923077, 54.359999999999985, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2]'),\n",
       " Text(309.04615384615386, 90.6, 'gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02666667, 0.        , 0.55072262, 0.42261071])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble\n",
    "- 여러개의 모델을 함께 사용하여 성능이 좋음\n",
    "- 과접합 방지 \n",
    "    - 많은 대회에서 ensemble기법 사용(정확도가 속도보다 중요)\n",
    "    \n",
    "    - 단일 모델의 문제점:\n",
    "        - 성능(voting,boosting, stacking)\n",
    "        - overfitting(bagging)\n",
    "    \n",
    "### VotingClassifier\n",
    "- ensemble의 일종으로 같은 데이터로 종류가 다른 모델로 voting 방식으로 함\n",
    "    - hard voting :다수결\n",
    "        - 성능에 상관없이 각 모델별로 동인한 표\n",
    "    - soft voting : 전체 확률 합을 구하여 큰 값을 선택\n",
    "-  VotingClassifier( (str, estimator) ) #pipeline과 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=VotingClassifier([   ('lr',LogisticRegression()),\n",
    "                        ('dt',DecisionTreeClassifier()),\n",
    "                        ('knn', KNeighborsClassifier())\n",
    "                    ],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MG/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('dt', DecisionTreeClassifier()),\n",
       "                             ('knn', KNeighborsClassifier())],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.fit(iris.iloc[:,:-1],iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['virginica'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.predict([[3,3,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimators': [('lr', LogisticRegression()),\n",
       "  ('dt', DecisionTreeClassifier()),\n",
       "  ('knn', KNeighborsClassifier())],\n",
       " 'flatten_transform': True,\n",
       " 'n_jobs': None,\n",
       " 'verbose': False,\n",
       " 'voting': 'soft',\n",
       " 'weights': None,\n",
       " 'lr': LogisticRegression(),\n",
       " 'dt': DecisionTreeClassifier(),\n",
       " 'knn': KNeighborsClassifier(),\n",
       " 'lr__C': 1.0,\n",
       " 'lr__class_weight': None,\n",
       " 'lr__dual': False,\n",
       " 'lr__fit_intercept': True,\n",
       " 'lr__intercept_scaling': 1,\n",
       " 'lr__l1_ratio': None,\n",
       " 'lr__max_iter': 100,\n",
       " 'lr__multi_class': 'auto',\n",
       " 'lr__n_jobs': None,\n",
       " 'lr__penalty': 'l2',\n",
       " 'lr__random_state': None,\n",
       " 'lr__solver': 'lbfgs',\n",
       " 'lr__tol': 0.0001,\n",
       " 'lr__verbose': 0,\n",
       " 'lr__warm_start': False,\n",
       " 'dt__ccp_alpha': 0.0,\n",
       " 'dt__class_weight': None,\n",
       " 'dt__criterion': 'gini',\n",
       " 'dt__max_depth': None,\n",
       " 'dt__max_features': None,\n",
       " 'dt__max_leaf_nodes': None,\n",
       " 'dt__min_impurity_decrease': 0.0,\n",
       " 'dt__min_impurity_split': None,\n",
       " 'dt__min_samples_leaf': 1,\n",
       " 'dt__min_samples_split': 2,\n",
       " 'dt__min_weight_fraction_leaf': 0.0,\n",
       " 'dt__presort': 'deprecated',\n",
       " 'dt__random_state': None,\n",
       " 'dt__splitter': 'best',\n",
       " 'knn__algorithm': 'auto',\n",
       " 'knn__leaf_size': 30,\n",
       " 'knn__metric': 'minkowski',\n",
       " 'knn__metric_params': None,\n",
       " 'knn__n_jobs': None,\n",
       " 'knn__n_neighbors': 5,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_n_features',\n",
       " '_collect_probas',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_params',\n",
       " '_get_tags',\n",
       " '_log_message',\n",
       " '_more_tags',\n",
       " '_predict',\n",
       " '_predict_proba',\n",
       " '_replace_estimator',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_required_parameters',\n",
       " '_set_params',\n",
       " '_sk_visual_block_',\n",
       " '_validate_data',\n",
       " '_validate_estimators',\n",
       " '_validate_names',\n",
       " '_weights_not_none',\n",
       " 'classes_',\n",
       " 'estimators',\n",
       " 'estimators_',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'flatten_transform',\n",
       " 'get_params',\n",
       " 'le_',\n",
       " 'n_features_in_',\n",
       " 'n_jobs',\n",
       " 'named_estimators',\n",
       " 'named_estimators_',\n",
       " 'predict',\n",
       " 'predict_proba',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'transform',\n",
       " 'verbose',\n",
       " 'voting',\n",
       " 'weights']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ClassifierMixin.score of VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('dt', DecisionTreeClassifier()),\n",
       "                             ('knn', KNeighborsClassifier())],\n",
       "                 voting='soft')>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimators': [('lr', LogisticRegression()),\n",
       "  ('dt', DecisionTreeClassifier()),\n",
       "  ('knn', KNeighborsClassifier())],\n",
       " 'voting': 'soft',\n",
       " 'weights': None,\n",
       " 'n_jobs': None,\n",
       " 'flatten_transform': True,\n",
       " 'verbose': False,\n",
       " 'le_': LabelEncoder(),\n",
       " 'classes_': array(['setosa', 'versicolor', 'virginica'], dtype=object),\n",
       " 'estimators_': [LogisticRegression(),\n",
       "  DecisionTreeClassifier(),\n",
       "  KNeighborsClassifier()],\n",
       " 'named_estimators_': {'lr': LogisticRegression(),\n",
       "  'dt': DecisionTreeClassifier(),\n",
       "  'knn': KNeighborsClassifier()}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(vc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.81796729e-01, 1.82032562e-02, 1.44268498e-08, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.71724746e-01, 2.82752243e-02, 3.01656728e-08, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.85443803e-01, 1.45561843e-02, 1.23261377e-08, ...,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [1.36439778e-04, 1.56811416e-01, 8.43052144e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [4.48444383e-05, 3.84985886e-02, 9.61456567e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "       [4.68278401e-04, 2.35056640e-01, 7.64475082e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.transform(iris.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56906732, 0.16402821, 0.26690447, 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.8       , 0.2       ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc.transform([[3,3,3,3]]) #결과 9개 (각 모델별 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MG/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(iris.iloc[:,:-1],iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각각에 대해서 확률로 표현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56906732, 0.16402821, 0.26690447]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba([[3,3,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56375654, -1.80771688, -1.32086446]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_log_proba([[3,3,3,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic regression 은 함수를 찾으므로  decision_function\n",
    "    - simoid/ logit = $\\frac{1}{1+e^{-x}}$\n",
    "- decision tree  은 함수가 없고 information gain/gini 기반으로 확률로 예측 predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.8, 0.2]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(iris.iloc[:,:-1],iris.species)\n",
    "knn.predict_proba([[3,3,3,3]]) #1 4개,2 1개 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66702275, -0.57693759, -0.09008516]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.decision_function([[3,3,3,3]]) #각 클래스별 confidence socre 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "- ensemble의 또 다른 방법인 \n",
    "- bagging(Bootstrap Aggregation )\n",
    "    - 랜덤 복원 샘플링해서 여러 모델은 만들어 평균울 내어 overfitting 방지 \n",
    "\n",
    "### Bagging\n",
    "- decision tree 기본 설정\n",
    "- Bootstrap Aggregation )\n",
    "    - 주어진 데이터에서 **랜덤 복원 샘플링**해서 모델을 여러개 만들어 평균울 냄\n",
    "    - 편차를 줄여서 **overfitting 방지**( 일반화)\n",
    "> Statistical Modeling : The Two Cultures: https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726\n",
    "    - decision tree 기반\n",
    "    - CART \n",
    "    - Random Forest : 나무가 모이면 숲:\n",
    "- parametric model \n",
    "    - y=ax+b 1차식\n",
    "\n",
    "데이터 기반으로 가장 좋은 방법을 찾음\n",
    "- black box model: 설명 불가해도 성능이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg=BaggingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.fit(iris.iloc[:,:-1],iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier\n",
    "- decision tree기반으로 각각의 데이터를 여러트리에 동시에 집어 넣어서 voting으로 최종결과를 냄\n",
    "-  overfitting방지, 성능 향상\n",
    "> ensemble끼리 ensemble시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  주어진 데이터에서 일부 복원 추출하므로 데이터 크기가 작아 성능 향상은 크지 않아서 과적합에 적합\n",
    "\n",
    "### boosting\n",
    "- 랜덤 복원 샘플링해서 여러 모델은 만들어 분류 잘 못하는 모델에 가중치를 주어 성능이 좋아짐 \n",
    "- Eg. XGBoost, lightGBM(Gradient Boosting Machine) over fitting도 방지하면서 성능 향상, 학습 속도 저하\n",
    "\n",
    "Big data Availability\n",
    "New ML technique\n",
    "GPU Acceleration\n",
    "\n",
    "### AdaBoostClassifier\n",
    "- 기본적으로 decision tree 계열\n",
    "- parametric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=AdaBoostClassifier(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking\n",
    "- kaggle ensemble guide 가장 우승을 많이함\n",
    "- 모델을 여러개 만들기 때문에 속도가 느림 \n",
    "- StackingClassifier( estimators, final_estimator=None(Logistic,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StackingClassifier([('knn',KNeighborsClassifier()),('dt',DecisionTreeClassifier()), ('svm',SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('knn', KNeighborsClassifier()),\n",
       "                               ('dt', DecisionTreeClassifier()),\n",
       "                               ('svm', SVC())])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.fit(iris.iloc[:,:-1],iris.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['versicolor'], dtype=object)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.predict([[3,3,3,3,]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.16149727,  1.82989138,  0.33160589]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.decision_function([[3,3,3,3,]]) #최종 모델로 logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01487458, 0.80516135, 0.17996407]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.predict_proba([[3,3,3,3]]) #1번째 가장큼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.8       ,  0.2       ,  0.        ,  0.        ,\n",
       "         1.        , -0.11414956,  2.12314294,  0.97963517]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.transform([[3,3,3,3,]]) #각 모델별로 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
